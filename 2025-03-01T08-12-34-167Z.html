<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
    <link rel="stylesheet" href="./style.css" />
  </head>

  <body class="markdown-body">
    <header class="container">
      <!-- * title * -->
      <h1>
        Optimizing Vector Dimensions: PCA, Similarity Search, and Elasticsearch
        Alternatives
      </h1>
    </header>

    <div class="content container">
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-2"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="f202a4bb-decd-4ea6-b334-7ae6a174caf9"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          What is a vector db?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-3"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="7cf5129a-5aa1-4bb0-bdd4-345b9c8c4de8"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <p data-start="0" data-end="311">
                          A
                          <strong data-start="2" data-end="33"
                            >Vector Database (Vector DB)</strong
                          >
                          is a specialized type of database designed to store,
                          index, and retrieve
                          <strong data-start="107" data-end="128"
                            >vector embeddings</strong
                          >
                          efficiently. These embeddings are numerical
                          representations of data (such as text, images, audio,
                          or video) in high-dimensional space, enabling fast and
                          accurate similarity searches.
                        </p>
                        <h3 data-start="313" data-end="341">
                          <strong data-start="317" data-end="341"
                            >Why Use a Vector DB?</strong
                          >
                        </h3>
                        <p data-start="342" data-end="510">
                          Traditional databases (SQL, NoSQL) are not optimized
                          for searching
                          <strong data-start="409" data-end="423"
                            >similarity</strong
                          >
                          between high-dimensional vectors. A Vector DB is
                          designed specifically for tasks like:
                        </p>
                        <ul data-start="512" data-end="809">
                          <li data-start="512" data-end="587">
                            <strong data-start="514" data-end="533"
                              >Semantic Search</strong
                            >
                            (e.g., searching text based on meaning, not
                            keywords)
                          </li>
                          <li data-start="588" data-end="654">
                            <strong data-start="590" data-end="614"
                              >Image &amp; Video Search</strong
                            >
                            (e.g., finding visually similar images)
                          </li>
                          <li data-start="655" data-end="734">
                            <strong data-start="657" data-end="683"
                              >Recommendation Systems</strong
                            >
                            (e.g., suggesting products based on user behavior)
                          </li>
                          <li data-start="735" data-end="809">
                            <strong data-start="737" data-end="758"
                              >Anomaly Detection</strong
                            >
                            (e.g., spotting unusual patterns in cybersecurity)
                          </li>
                        </ul>
                        <h3 data-start="811" data-end="831">
                          <strong data-start="815" data-end="831"
                            >How It Works</strong
                          >
                        </h3>
                        <ol data-start="832" data-end="1271">
                          <li data-start="832" data-end="975">
                            <strong data-start="835" data-end="854"
                              >Data Conversion</strong
                            >
                            – Raw data (e.g., text, image) is converted into
                            <strong data-start="904" data-end="925"
                              >vector embeddings</strong
                            >
                            using AI models (like OpenAI’s CLIP, BERT, etc.).
                          </li>
                          <li data-start="976" data-end="1091">
                            <strong data-start="979" data-end="1001"
                              >Storage &amp; Indexing</strong
                            >
                            – These embeddings are stored in the vector database
                            and indexed for efficient retrieval.
                          </li>
                          <li data-start="1092" data-end="1271">
                            <strong data-start="1095" data-end="1116"
                              >Similarity Search</strong
                            >
                            – When a query vector is given, the DB finds the
                            <strong data-start="1166" data-end="1185"
                              >nearest vectors</strong
                            >
                            (using techniques like
                            <strong data-start="1209" data-end="1232"
                              >FAISS, HNSW, or ANN</strong
                            >), returning the most relevant results.
                          </li>
                        </ol>
                        <h3 data-start="1273" data-end="1305">
                          <strong data-start="1277" data-end="1305"
                            >Popular Vector Databases</strong
                          >
                        </h3>
                        <ul data-start="1306" data-end="1672">
                          <li data-start="1306" data-end="1407">
                            <strong data-start="1308" data-end="1317"
                              >FAISS</strong
                            >
                            (Facebook AI Similarity Search) – Open-source,
                            optimized for large-scale vector searches.
                          </li>
                          <li data-start="1408" data-end="1469">
                            <strong data-start="1410" data-end="1422"
                              >Pinecone</strong
                            >
                            – Managed vector DB with high-speed retrieval.
                          </li>
                          <li data-start="1470" data-end="1542">
                            <strong data-start="1472" data-end="1484"
                              >Weaviate</strong
                            >
                            – Open-source, supports hybrid search (vector +
                            keyword).
                          </li>
                          <li data-start="1543" data-end="1608">
                            <strong data-start="1545" data-end="1555"
                              >Milvus</strong
                            >
                            – Scalable and high-performance for AI applications.
                          </li>
                          <li data-start="1609" data-end="1672">
                            <strong data-start="1611" data-end="1620"
                              >Vespa</strong
                            >
                            – Supports both structured and unstructured search.
                          </li>
                        </ul>
                        <h3 data-start="1674" data-end="1695">
                          <strong data-start="1678" data-end="1695"
                            >Key Takeaways</strong
                          >
                        </h3>
                        <ul data-start="1696" data-end="1985">
                          <li data-start="1696" data-end="1801">
                            Vector DBs
                            <strong data-start="1709" data-end="1737"
                              >power AI-driven searches</strong
                            >
                            by handling
                            <strong data-start="1750" data-end="1788"
                              >high-dimensional vector similarity</strong
                            >
                            efficiently.
                          </li>
                          <li data-start="1802" data-end="1887">
                            They are
                            <strong data-start="1813" data-end="1828"
                              >much faster</strong
                            >
                            than traditional databases for searching
                            <strong data-start="1870" data-end="1886"
                              >similar data</strong
                            >.
                          </li>
                          <li data-start="1888" data-end="1985">
                            Essential for applications in
                            <strong data-start="1920" data-end="1984"
                              >AI, search engines, recommendations, and pattern
                              recognition</strong
                            >.
                          </li>
                        </ul>
                        <p
                          data-start="1987"
                          data-end="2050"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like an example of how to use a Vector DB in
                          code? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-4"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="f48bebc6-6597-4c68-953d-c82a91eb672e"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          How is data conversion handled?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-5"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="951d02a5-e4f7-4500-843b-ab60c8c6bf4a"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="54">
                          <strong data-start="4" data-end="54"
                            >How Data Conversion Works in a Vector
                            Database</strong
                          >
                        </h3>
                        <p data-start="55" data-end="309">
                          Before data can be stored in a
                          <strong data-start="86" data-end="105"
                            >vector database</strong
                          >, it must first be
                          <strong data-start="124" data-end="160"
                            >converted into vector embeddings</strong
                          >. This is done using
                          <strong data-start="181" data-end="208"
                            >machine learning models</strong
                          >
                          that transform raw data (text, images, audio, etc.)
                          into high-dimensional numerical representations.
                        </p>
                        <hr data-start="311" data-end="314" />
                        <h2 data-start="316" data-end="351">
                          <strong data-start="319" data-end="351"
                            >1. Text to Vector Conversion</strong
                          >
                        </h2>
                        <p data-start="352" data-end="495">
                          For
                          <strong data-start="356" data-end="370"
                            >text-based</strong
                          >
                          applications (e.g., search engines, chatbots,
                          recommendation systems), words or sentences need to be
                          embedded into vectors.
                        </p>
                        <h3 data-start="497" data-end="520">
                          <strong data-start="501" data-end="520"
                            >Common Methods:</strong
                          >
                        </h3>
                        <ul data-start="521" data-end="873">
                          <li data-start="521" data-end="622">
                            <strong data-start="523" data-end="542"
                              >Word Embeddings</strong
                            >: Converts words into fixed-size vectors.
                            <ul data-start="586" data-end="622">
                              <li data-start="586" data-end="622">
                                Example: Word2Vec, GloVe, FastText
                              </li>
                            </ul>
                          </li>
                          <li data-start="623" data-end="769">
                            <strong data-start="625" data-end="659"
                              >Sentence &amp; Document Embeddings</strong
                            >: Captures
                            <strong data-start="670" data-end="695"
                              >context and semantics</strong
                            >
                            of a sentence.
                            <ul data-start="713" data-end="769">
                              <li data-start="713" data-end="769">
                                Example: BERT, OpenAI’s Ada, Sentence-BERT
                                (SBERT), T5
                              </li>
                            </ul>
                          </li>
                          <li data-start="770" data-end="873">
                            <strong data-start="772" data-end="802"
                              >TF-IDF + Sparse Embeddings</strong
                            >: Uses traditional keyword frequency analysis along
                            with vector search.
                          </li>
                        </ul>
                        <h4 data-start="875" data-end="923">
                          <strong data-start="880" data-end="923"
                            >Example (Using OpenAI’s Embeddings API)</strong
                          >
                        </h4>
                        <pre
                          class="!overflow-visible"
                          data-start="924"
                          data-end="1231"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from openai import OpenAI

client = OpenAI(api_key="your_api_key")

response = client.embeddings.create(
    model="text-embedding-ada-002",
    input="How does a vector database work?"
)
vector = response['data'][0]['embedding']
print(vector[:5])  # Prints first 5 dimensions of the embedding
</span></code></div></div></pre>
                        <hr data-start="1233" data-end="1236" />
                        <h2 data-start="1238" data-end="1275">
                          <strong data-start="1241" data-end="1275"
                            >2. Images to Vector Conversion</strong
                          >
                        </h2>
                        <p data-start="1276" data-end="1423">
                          For
                          <strong data-start="1280" data-end="1295"
                            >image-based</strong
                          >
                          applications (e.g., reverse image search, object
                          recognition), deep learning models convert images into
                          vector representations.
                        </p>
                        <h3 data-start="1425" data-end="1448">
                          <strong data-start="1429" data-end="1448"
                            >Common Methods:</strong
                          >
                        </h3>
                        <ul data-start="1449" data-end="1633">
                          <li data-start="1449" data-end="1535">
                            <strong data-start="1451" data-end="1471"
                              >CNN-Based Models</strong
                            >: Extract image features.
                            <ul data-start="1499" data-end="1535">
                              <li data-start="1499" data-end="1535">
                                Example: ResNet, VGG, EfficientNet
                              </li>
                            </ul>
                          </li>
                          <li data-start="1536" data-end="1633">
                            <strong data-start="1538" data-end="1560"
                              >Multi-Modal Models</strong
                            >: Combine text + image understanding.
                            <ul data-start="1600" data-end="1633">
                              <li data-start="1600" data-end="1633">
                                Example: CLIP (by OpenAI), DINO
                              </li>
                            </ul>
                          </li>
                        </ul>
                        <h4 data-start="1635" data-end="1696">
                          <strong data-start="1640" data-end="1696"
                            >Example (Using CLIP to Convert an Image to a
                            Vector)</strong
                          >
                        </h4>
                        <pre
                          class="!overflow-visible"
                          data-start="1697"
                          data-end="2145"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import torch
import clip
from PIL import Image

# Load the model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Load and process image
image = preprocess(Image.open("example.jpg")).unsqueeze(0).to(device)

# Generate vector embedding
with torch.no_grad():
    image_embedding = model.encode_image(image)

print(image_embedding.shape)  # Output: torch.Size([1, 512])
</span></code></div></div></pre>
                        <hr data-start="2147" data-end="2150" />
                        <h2 data-start="2152" data-end="2188">
                          <strong data-start="2155" data-end="2188"
                            >3. Audio to Vector Conversion</strong
                          >
                        </h2>
                        <p data-start="2189" data-end="2342">
                          For
                          <strong data-start="2193" data-end="2208"
                            >audio-based</strong
                          >
                          applications (e.g., voice search, speaker
                          recognition), raw audio is first converted into a
                          spectrogram, then embedded into a vector.
                        </p>
                        <h3 data-start="2344" data-end="2367">
                          <strong data-start="2348" data-end="2367"
                            >Common Methods:</strong
                          >
                        </h3>
                        <ul data-start="2368" data-end="2613">
                          <li data-start="2368" data-end="2448">
                            <strong data-start="2370" data-end="2386"
                              >Wave2Vec 2.0</strong
                            >
                            (by Facebook) – Learns speech representations from
                            raw audio.
                          </li>
                          <li data-start="2449" data-end="2513">
                            <strong data-start="2451" data-end="2471"
                              >OpenAI’s Whisper</strong
                            >
                            – Converts speech to text and embeddings.
                          </li>
                          <li data-start="2514" data-end="2613">
                            <strong data-start="2516" data-end="2562"
                              >MFCC (Mel-Frequency Cepstral
                              Coefficients)</strong
                            >
                            – Traditional method for voice feature extraction.
                          </li>
                        </ul>
                        <h4 data-start="2615" data-end="2674">
                          <strong data-start="2620" data-end="2674"
                            >Example (Using OpenAI Whisper for Audio
                            Embedding)</strong
                          >
                        </h4>
                        <pre
                          class="!overflow-visible"
                          data-start="2675"
                          data-end="2860"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import whisper

model = whisper.load_model("base")
audio_embedding = model.encode("example.wav")
print(audio_embedding.shape)  # Output: (number of tokens, embedding size)
</span></code></div></div></pre>
                        <hr data-start="2862" data-end="2865" />
                        <h2 data-start="2867" data-end="2913">
                          <strong data-start="2870" data-end="2913"
                            >4. Structured &amp; Tabular Data to Vectors</strong
                          >
                        </h2>
                        <p data-start="2914" data-end="2982">
                          For
                          <strong data-start="2918" data-end="2937"
                            >structured data</strong
                          >
                          (like relational databases), you can either:
                        </p>
                        <ol data-start="2983" data-end="3244">
                          <li data-start="2983" data-end="3067">
                            <strong data-start="2986" data-end="3013"
                              >Use feature engineering</strong
                            >
                            (e.g., scaling numerical features into vector
                            space).
                          </li>
                          <li data-start="3068" data-end="3143">
                            <strong data-start="3071" data-end="3095"
                              >Train an Autoencoder</strong
                            >
                            to convert tabular data into vector embeddings.
                          </li>
                          <li data-start="3144" data-end="3244">
                            <strong data-start="3147" data-end="3193"
                              >Combine structured data with deep
                              learning</strong
                            >
                            (e.g., categorical embeddings in neural networks).
                          </li>
                        </ol>
                        <h4 data-start="3246" data-end="3308">
                          <strong data-start="3251" data-end="3308"
                            >Example (Using PCA to Reduce Tabular Data to
                            Vectors)</strong
                          >
                        </h4>
                        <pre
                          class="!overflow-visible"
                          data-start="3309"
                          data-end="3602"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from sklearn.decomposition import PCA
import numpy as np

data = np.random.rand(100, 10)  # Example tabular data (100 rows, 10 features)
pca = PCA(n_components=5)  # Reduce to 5 dimensions
vectorized_data = pca.fit_transform(data)
print(vectorized_data.shape)  # Output: (100, 5)
</span></code></div></div></pre>
                        <hr data-start="3604" data-end="3607" />
                        <h2 data-start="3609" data-end="3663">
                          <strong data-start="3612" data-end="3663"
                            >5. Storing &amp; Querying the Vector in a Vector
                            DB</strong
                          >
                        </h2>
                        <p data-start="3664" data-end="3800">
                          After converting the data, it is stored in a
                          <strong data-start="3709" data-end="3728"
                            >vector database</strong
                          >
                          like
                          <strong data-start="3734" data-end="3764"
                            >FAISS, Pinecone, or Milvus</strong
                          >. Here’s an example using
                          <strong data-start="3790" data-end="3799"
                            >FAISS</strong
                          >:
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="3802"
                          data-end="4282"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import faiss
import numpy as np

# Sample 100 vectors of size 128
vector_data = np.random.rand(100, 128).astype('float32')

# Create an index
index = faiss.IndexFlatL2(128)  # L2 distance (Euclidean)
index.add(vector_data)  # Add vectors to the database

# Query the nearest neighbor
query_vector = np.random.rand(1, 128).astype('float32')
_, result_indices = index.search(query_vector, 5)  # Get 5 closest vectors
print(result_indices)  # Indices of closest vectors
</span></code></div></div></pre>
                        <hr data-start="4284" data-end="4287" />
                        <h2 data-start="4289" data-end="4310">
                          <strong data-start="4292" data-end="4310"
                            >Final Thoughts</strong
                          >
                        </h2>
                        <p data-start="4311" data-end="4550">
                          🔹
                          <strong data-start="4314" data-end="4373"
                            >Data conversion is the first step in using a Vector
                            DB.</strong
                          ><br data-start="4373" data-end="4376" />🔹 Different
                          AI models convert text, images, audio, and structured
                          data into
                          <strong data-start="4453" data-end="4474"
                            >vector embeddings</strong
                          >.<br data-start="4475" data-end="4478" />🔹 These
                          embeddings are then stored, indexed, and queried
                          efficiently.
                        </p>
                        <p
                          data-start="4552"
                          data-end="4622"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like help setting up a Vector DB for a
                          specific use case? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-6"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="f35c5dde-cfec-4d7f-8f19-2e96f8315d1b"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          What other options do I have instead of AI models in
                          order to handle data conversion?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-7"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="d088f4da-1ad8-4dc9-9360-be4366390c05"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <p data-start="0" data-end="308">
                          If you don’t want to use AI models to convert data
                          into vector embeddings, you can explore
                          <strong data-start="91" data-end="114"
                            >alternative methods</strong
                          >
                          based on
                          <strong data-start="124" data-end="180"
                            >mathematical, statistical, or algorithmic
                            techniques</strong
                          >. These approaches may not capture semantic meaning
                          as well as AI models, but they can still be effective
                          for certain use cases.
                        </p>
                        <hr data-start="310" data-end="313" />
                        <h2 data-start="315" data-end="351">
                          <strong data-start="318" data-end="351"
                            >1. Manual Feature Engineering</strong
                          >
                        </h2>
                        <p data-start="352" data-end="459">
                          Instead of AI-generated embeddings, you can manually
                          extract features from the data using domain knowledge.
                        </p>
                        <h3 data-start="461" data-end="491">
                          <strong data-start="465" data-end="491"
                            >Example for Text Data:</strong
                          >
                        </h3>
                        <ul data-start="492" data-end="723">
                          <li data-start="492" data-end="562">
                            <strong data-start="494" data-end="517"
                              >Bag of Words (BoW):</strong
                            >
                            Converts text into a frequency-based vector.
                          </li>
                          <li data-start="563" data-end="654">
                            <strong data-start="565" data-end="620"
                              >TF-IDF (Term Frequency-Inverse Document
                              Frequency):</strong
                            >
                            Weighs words based on importance.
                          </li>
                          <li data-start="655" data-end="720">
                            <strong data-start="657" data-end="669"
                              >n-Grams:</strong
                            >
                            Captures word sequences (e.g., bigrams, trigrams).
                          </li>
                        </ul>
                        <p data-start="724" data-end="771">
                          📌
                          <strong data-start="727" data-end="771"
                            >Example (TF-IDF Vectorization in Python)</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="772"
                          data-end="1049"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from sklearn.feature_extraction.text import TfidfVectorizer

texts = ["vector databases are fast", "databases store information"]
vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(texts)
print(vectors.toarray())  # Prints a matrix of TF-IDF values
</span></code></div></div></pre>
                        <p data-start="1050" data-end="1171">
                          <strong data-start="1050" data-end="1059"
                            >Pros:</strong
                          >
                          Simple, interpretable, computationally efficient.<br
                            data-start="1109"
                            data-end="1112"
                          /><strong data-start="1112" data-end="1121"
                            >Cons:</strong
                          >
                          Doesn’t capture meaning, lacks context awareness.
                        </p>
                        <hr data-start="1173" data-end="1176" />
                        <h2 data-start="1178" data-end="1223">
                          <strong data-start="1181" data-end="1223"
                            >2. Dimensionality Reduction Techniques</strong
                          >
                        </h2>
                        <p data-start="1224" data-end="1310">
                          You can transform structured/tabular data into vector
                          form using mathematical methods.
                        </p>
                        <h3 data-start="1312" data-end="1335">
                          <strong data-start="1316" data-end="1335"
                            >Common Methods:</strong
                          >
                        </h3>
                        <ul data-start="1336" data-end="1593">
                          <li data-start="1336" data-end="1424">
                            <strong data-start="1338" data-end="1376"
                              >Principal Component Analysis (PCA)</strong
                            >
                            – Reduces dimensions while preserving variance.
                          </li>
                          <li data-start="1425" data-end="1513">
                            <strong data-start="1427" data-end="1482"
                              >t-SNE (t-Distributed Stochastic Neighbor
                              Embedding)</strong
                            >
                            – Captures complex structures.
                          </li>
                          <li data-start="1514" data-end="1593">
                            <strong data-start="1516" data-end="1572"
                              >UMAP (Uniform Manifold Approximation and
                              Projection)</strong
                            >
                            – Faster than t-SNE.
                          </li>
                        </ul>
                        <p data-start="1595" data-end="1651">
                          📌
                          <strong data-start="1598" data-end="1651"
                            >Example (Using PCA to Reduce a 10D Dataset to
                            3D)</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1652"
                          data-end="1872"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from sklearn.decomposition import PCA
import numpy as np

data = np.random.rand(100, 10)  # 100 rows, 10 features
pca = PCA(n_components=3)
vectors = pca.fit_transform(data)
print(vectors.shape)  # (100, 3)
</span></code></div></div></pre>
                        <p data-start="1873" data-end="2010">
                          <strong data-start="1873" data-end="1882"
                            >Pros:</strong
                          >
                          Works well for numerical data, reduces storage
                          needs.<br data-start="1936" data-end="1939" /><strong
                            data-start="1939"
                            data-end="1948"
                            >Cons:</strong
                          >
                          May lose some information, not suitable for
                          non-numeric data.
                        </p>
                        <hr data-start="2012" data-end="2015" />
                        <h2 data-start="2017" data-end="2045">
                          <strong data-start="2020" data-end="2045"
                            >3. Hashing Techniques</strong
                          >
                        </h2>
                        <p data-start="2046" data-end="2150">
                          Instead of AI embeddings, you can use
                          <strong data-start="2084" data-end="2102"
                            >hash functions</strong
                          >
                          to convert data into numerical representations.
                        </p>
                        <h3 data-start="2152" data-end="2175">
                          <strong data-start="2156" data-end="2175"
                            >Common Methods:</strong
                          >
                        </h3>
                        <ul data-start="2176" data-end="2416">
                          <li data-start="2176" data-end="2262">
                            <strong data-start="2178" data-end="2189"
                              >MinHash</strong
                            >
                            – Efficient for similarity search (e.g., detecting
                            duplicate documents).
                          </li>
                          <li data-start="2263" data-end="2347">
                            <strong data-start="2265" data-end="2301"
                              >LSH (Locality-Sensitive Hashing)</strong
                            >
                            – Maps similar items to the same hash bucket.
                          </li>
                          <li data-start="2348" data-end="2416">
                            <strong data-start="2350" data-end="2361"
                              >SimHash</strong
                            >
                            – Used in search engines for near-duplicate
                            detection.
                          </li>
                        </ul>
                        <p data-start="2418" data-end="2464">
                          📌
                          <strong data-start="2421" data-end="2464"
                            >Example (Using LSH for Text Similarity)</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="2465"
                          data-end="2815"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from datasketch import MinHash

text1 = "vector databases are efficient"
text2 = "databases store vectorized data"

m1, m2 = MinHash(), MinHash()
for word in text1.split():
    m1.update(word.encode('utf8'))
for word in text2.split():
    m2.update(word.encode('utf8'))

print(m1.jaccard(m2))  # Output: similarity score between 0 and 1
</span></code></div></div></pre>
                        <p data-start="2816" data-end="2957">
                          <strong data-start="2816" data-end="2825"
                            >Pros:</strong
                          >
                          Fast, memory-efficient, great for
                          <strong data-start="2860" data-end="2898"
                            >approximate nearest neighbor (ANN)</strong
                          >
                          search.<br data-start="2906" data-end="2909" /><strong
                            data-start="2909"
                            data-end="2918"
                            >Cons:</strong
                          >
                          Less precise than AI-based embeddings.
                        </p>
                        <hr data-start="2959" data-end="2962" />
                        <h2 data-start="2964" data-end="3015">
                          <strong data-start="2967" data-end="3015"
                            >4. Statistical Encoding for Categorical
                            Data</strong
                          >
                        </h2>
                        <p data-start="3016" data-end="3111">
                          If you have categorical (non-numeric) data, you can
                          convert it into numerical vectors manually.
                        </p>
                        <h3 data-start="3113" data-end="3136">
                          <strong data-start="3117" data-end="3136"
                            >Common Methods:</strong
                          >
                        </h3>
                        <ul data-start="3137" data-end="3356">
                          <li data-start="3137" data-end="3202">
                            <strong data-start="3139" data-end="3159"
                              >One-Hot Encoding</strong
                            >
                            – Converts categories into binary vectors.
                          </li>
                          <li data-start="3203" data-end="3270">
                            <strong data-start="3205" data-end="3225"
                              >Ordinal Encoding</strong
                            >
                            – Assigns a rank-based number to categories.
                          </li>
                          <li data-start="3271" data-end="3356">
                            <strong data-start="3273" data-end="3292"
                              >Target Encoding</strong
                            >
                            – Uses statistical relations between category and
                            target value.
                          </li>
                        </ul>
                        <p data-start="3358" data-end="3401">
                          📌
                          <strong data-start="3361" data-end="3401"
                            >Example (One-Hot Encoding in Pandas)</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="3402"
                          data-end="3546"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import pandas as pd

df = pd.DataFrame({'Category': ['A', 'B', 'C', 'A']})
one_hot = pd.get_dummies(df['Category'])
print(one_hot)
</span></code></div></div></pre>
                        <p data-start="3547" data-end="3660">
                          <strong data-start="3547" data-end="3556"
                            >Pros:</strong
                          >
                          Simple, effective for structured data.<br
                            data-start="3595"
                            data-end="3598"
                          /><strong data-start="3598" data-end="3607"
                            >Cons:</strong
                          >
                          Can lead to large vectors for high-cardinality data.
                        </p>
                        <hr data-start="3662" data-end="3665" />
                        <h2 data-start="3667" data-end="3709">
                          <strong data-start="3670" data-end="3709"
                            >5. Signal Processing for Audio Data</strong
                          >
                        </h2>
                        <p data-start="3710" data-end="3844">
                          If you want to extract numerical features from
                          <strong data-start="3757" data-end="3766"
                            >audio</strong
                          >
                          without AI models, you can use
                          <strong data-start="3798" data-end="3820"
                            >Fourier transforms</strong
                          >
                          or statistical methods.
                        </p>
                        <h3 data-start="3846" data-end="3869">
                          <strong data-start="3850" data-end="3869"
                            >Common Methods:</strong
                          >
                        </h3>
                        <ul data-start="3870" data-end="4102">
                          <li data-start="3870" data-end="3951">
                            <strong data-start="3872" data-end="3919"
                              >Mel-Frequency Cepstral Coefficients
                              (MFCCs)</strong
                            >
                            – Extracts key speech features.
                          </li>
                          <li data-start="3952" data-end="4034">
                            <strong data-start="3954" data-end="3986"
                              >Fast Fourier Transform (FFT)</strong
                            >
                            – Converts audio signals into frequency domain.
                          </li>
                          <li data-start="4035" data-end="4102">
                            <strong data-start="4037" data-end="4065"
                              >Zero-Crossing Rate (ZCR)</strong
                            >
                            – Measures signal changes over time.
                          </li>
                        </ul>
                        <p data-start="4104" data-end="4164">
                          📌
                          <strong data-start="4107" data-end="4164"
                            >Example (Extracting MFCC Features from an Audio
                            File)</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="4165"
                          data-end="4360"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import librosa

audio_path = "audio.wav"
y, sr = librosa.load(audio_path)
mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
print(mfcc_features.shape)  # (13, time frames)
</span></code></div></div></pre>
                        <p data-start="4361" data-end="4488">
                          <strong data-start="4361" data-end="4370"
                            >Pros:</strong
                          >
                          Works well for
                          <strong data-start="4386" data-end="4408"
                            >speech recognition</strong
                          >
                          and
                          <strong data-start="4413" data-end="4437"
                            >audio classification</strong
                          >.<br data-start="4438" data-end="4441" /><strong
                            data-start="4441"
                            data-end="4450"
                            >Cons:</strong
                          >
                          Requires signal processing expertise.
                        </p>
                        <hr data-start="4490" data-end="4493" />
                        <h2 data-start="4495" data-end="4540">
                          <strong data-start="4498" data-end="4540"
                            >6. Image Feature Extraction Without AI</strong
                          >
                        </h2>
                        <p data-start="4541" data-end="4657">
                          Instead of deep learning, you can extract basic
                          <strong data-start="4589" data-end="4607"
                            >image features</strong
                          >
                          using traditional
                          <strong data-start="4626" data-end="4645"
                            >computer vision</strong
                          >
                          techniques.
                        </p>
                        <h3 data-start="4659" data-end="4682">
                          <strong data-start="4663" data-end="4682"
                            >Common Methods:</strong
                          >
                        </h3>
                        <ul data-start="4683" data-end="4878">
                          <li data-start="4683" data-end="4759">
                            <strong data-start="4685" data-end="4726"
                              >Histogram of Oriented Gradients (HOG)</strong
                            >
                            – Extracts shape-based features.
                          </li>
                          <li data-start="4760" data-end="4824">
                            <strong data-start="4762" data-end="4803"
                              >ORB (Oriented FAST and Rotated BRIEF)</strong
                            >
                            – Detects keypoints.
                          </li>
                          <li data-start="4825" data-end="4878">
                            <strong data-start="4827" data-end="4847"
                              >Color Histograms</strong
                            >
                            – Encodes color distributions.
                          </li>
                        </ul>
                        <p data-start="4880" data-end="4933">
                          📌
                          <strong data-start="4883" data-end="4933"
                            >Example (Using OpenCV to Extract HOG
                            Features)</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="4934"
                          data-end="5126"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import cv2
import numpy as np

img = cv2.imread("image.jpg", cv2.IMREAD_GRAYSCALE)
hog = cv2.HOGDescriptor()
features = hog.compute(img)
print(features.shape)  # Vectorized image
</span></code></div></div></pre>
                        <p data-start="5127" data-end="5239">
                          <strong data-start="5127" data-end="5136"
                            >Pros:</strong
                          >
                          Computationally faster than deep learning models.<br
                            data-start="5186"
                            data-end="5189"
                          /><strong data-start="5189" data-end="5198"
                            >Cons:</strong
                          >
                          Less accurate than CNN-based embeddings.
                        </p>
                        <hr data-start="5241" data-end="5244" />
                        <h2 data-start="5246" data-end="5294">
                          <strong data-start="5249" data-end="5294"
                            >7. Rule-Based or Symbolic Representations</strong
                          >
                        </h2>
                        <p data-start="5295" data-end="5379">
                          For
                          <strong data-start="5299" data-end="5320"
                            >some applications</strong
                          >, a
                          <strong data-start="5324" data-end="5348"
                            >logical set of rules</strong
                          >
                          can replace vector embeddings.
                        </p>
                        <h3 data-start="5381" data-end="5398">
                          <strong data-start="5385" data-end="5398"
                            >Examples:</strong
                          >
                        </h3>
                        <ul data-start="5399" data-end="5567">
                          <li data-start="5399" data-end="5484">
                            <strong data-start="5401" data-end="5428"
                              >Taxonomies &amp; Ontologies</strong
                            >
                            – Used in
                            <strong data-start="5439" data-end="5458"
                              >semantic search</strong
                            >
                            and
                            <strong data-start="5463" data-end="5483"
                              >knowledge graphs</strong
                            >.
                          </li>
                          <li data-start="5485" data-end="5567">
                            <strong data-start="5487" data-end="5520"
                              >Logic-Based Retrieval Systems</strong
                            >
                            – Uses predefined relationships instead of AI.
                          </li>
                        </ul>
                        <p data-start="5569" data-end="5705">
                          <strong data-start="5569" data-end="5590"
                            >Example Use Case:</strong
                          ><br data-start="5590" data-end="5593" />Instead of
                          embedding medical text, a rule-based system might
                          check for
                          <strong data-start="5664" data-end="5683"
                            >keyword matches</strong
                          >
                          in medical databases.
                        </p>
                        <hr data-start="5707" data-end="5710" />
                        <h2 data-start="5712" data-end="5733">
                          <strong data-start="5715" data-end="5733"
                            >Final Thoughts</strong
                          >
                        </h2>
                        <p data-start="5734" data-end="6095">
                          If you
                          <strong data-start="5741" data-end="5772"
                            >don’t want to use AI models</strong
                          >, you can still convert data into vectors using: ✔
                          <strong data-start="5823" data-end="5889"
                            >Statistical &amp; mathematical methods (PCA,
                            TF-IDF, LSH, MinHash)</strong
                          ><br data-start="5889" data-end="5892" />✔
                          <strong data-start="5894" data-end="5961"
                            >Manual feature engineering (One-hot encoding,
                            Histograms, MFCC)</strong
                          ><br data-start="5961" data-end="5964" />✔
                          <strong data-start="5966" data-end="6033"
                            >Traditional signal processing &amp; computer vision
                            (HOG, FFT, ZCR)</strong
                          ><br data-start="6033" data-end="6036" />✔
                          <strong data-start="6038" data-end="6093"
                            >Rule-based representations (Ontologies,
                            Taxonomies)</strong
                          >
                        </p>
                        <p
                          data-start="6097"
                          data-end="6164"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like a recommendation based on your specific
                          use case? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-8"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="59322a71-f25e-4407-aeb0-4b890fe13220"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          What about using elastic-search to handle this?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-9"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="7deb880c-a202-4394-9cea-bad9fcce8d32"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="65">
                          <strong data-start="4" data-end="65"
                            >Using Elasticsearch for Data Conversion and Vector
                            Search</strong
                          >
                        </h3>
                        <p data-start="66" data-end="232">
                          Yes!
                          <strong data-start="71" data-end="88"
                            >Elasticsearch</strong
                          >
                          (ES) is a great alternative to dedicated vector
                          databases when handling
                          <strong data-start="161" data-end="196"
                            >text, image, or structured data</strong
                          >
                          without fully relying on AI models.
                        </p>
                        <p data-start="234" data-end="485">
                          Elasticsearch is traditionally used for
                          <strong data-start="274" data-end="300"
                            >keyword-based searches</strong
                          >, but it
                          <strong data-start="309" data-end="340"
                            >also supports vector search</strong
                          >
                          via
                          <strong data-start="345" data-end="399"
                            >k-NN (k-Nearest Neighbors) and dense vector
                            fields</strong
                          >. This makes it a strong choice for hybrid search
                          (combining keyword + vector search).
                        </p>
                        <hr data-start="487" data-end="490" />
                        <h2 data-start="492" data-end="543">
                          <strong data-start="495" data-end="543"
                            >1. Handling Data Conversion in
                            Elasticsearch</strong
                          >
                        </h2>
                        <p data-start="544" data-end="725">
                          Elasticsearch doesn’t directly convert data into
                          vectors, but it provides
                          <strong data-start="618" data-end="662"
                            >ways to index and query them efficiently</strong
                          >. You can handle conversion manually before storing
                          data in ES.
                        </p>
                        <h3 data-start="727" data-end="789">
                          <strong data-start="731" data-end="789"
                            >Methods of Conversion Before Indexing in
                            Elasticsearch</strong
                          >
                        </h3>
                        <p data-start="790" data-end="1098">
                          ✔
                          <strong data-start="792" data-end="805"
                            >Text Data</strong
                          >: Convert text into vectors using
                          <strong data-start="839" data-end="876"
                            >TF-IDF, BM25, or dense embeddings</strong
                          ><br data-start="876" data-end="879" />✔
                          <strong data-start="881" data-end="895"
                            >Image Data</strong
                          >: Convert images into feature vectors before
                          indexing<br data-start="948" data-end="951" />✔
                          <strong data-start="953" data-end="967"
                            >Audio Data</strong
                          >: Convert spectrograms or statistical features into
                          vectors<br data-start="1026" data-end="1029" />✔
                          <strong data-start="1031" data-end="1050"
                            >Structured Data</strong
                          >: Store numerical features directly as vectors
                        </p>
                        <p data-start="1100" data-end="1205">
                          Once the data is vectorized, ES can store it as a
                          <strong data-start="1150" data-end="1172"
                            >dense vector field</strong
                          >
                          and perform similarity searches.
                        </p>
                        <hr data-start="1207" data-end="1210" />
                        <h2 data-start="1212" data-end="1275">
                          <strong data-start="1215" data-end="1275"
                            >2. Using TF-IDF &amp; BM25 for Text Search (Non-AI
                            Approach)</strong
                          >
                        </h2>
                        <p data-start="1276" data-end="1410">
                          If you want
                          <strong data-start="1288" data-end="1322"
                            >good text retrieval without AI</strong
                          >,
                          <strong data-start="1324" data-end="1332"
                            >BM25</strong
                          >
                          and
                          <strong data-start="1337" data-end="1347"
                            >TF-IDF</strong
                          >
                          scoring models in Elasticsearch can replace vector
                          embeddings.
                        </p>
                        <p data-start="1412" data-end="1454">
                          📌
                          <strong data-start="1415" data-end="1454"
                            >Example: Using BM25 for Text Search</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1455"
                          data-end="1760"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">json</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-json"><span>PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_analyzer": {
          "type": "standard"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "text": {
        "type": "text",
        "analyzer": "standard_analyzer"
      }
    }
  }
}
</span></code></div></div></pre>
                        <p data-start="1761" data-end="1901">
                          ✔
                          <strong data-start="1763" data-end="1777"
                            >Advantages</strong
                          >: Fast, well-optimized for traditional search<br
                            data-start="1822"
                            data-end="1825"
                          />❌
                          <strong data-start="1827" data-end="1842"
                            >Limitations</strong
                          >: Doesn't capture
                          <strong data-start="1860" data-end="1880"
                            >semantic meaning</strong
                          >
                          like AI embeddings
                        </p>
                        <hr data-start="1903" data-end="1906" />
                        <h2 data-start="1908" data-end="1960">
                          <strong data-start="1911" data-end="1960"
                            >3. Using Elasticsearch for k-NN Vector
                            Search</strong
                          >
                        </h2>
                        <p data-start="1961" data-end="2091">
                          If you
                          <strong data-start="1968" data-end="1993"
                            >precompute embeddings</strong
                          >
                          (e.g., using CLIP for images or Sentence-BERT for
                          text), Elasticsearch can store and search them.
                        </p>
                        <p data-start="2093" data-end="2147">
                          📌
                          <strong data-start="2096" data-end="2147"
                            >Example: Storing Dense Vectors in
                            Elasticsearch</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="2148"
                          data-end="2315"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">json</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-json"><span>PUT my_vector_index
{
  "mappings": {
    "properties": {
      "text_embedding": {
        "type": "dense_vector",
        "dims": 768
      }
    }
  }
}
</span></code></div></div></pre>
                        <h3 data-start="2316" data-end="2344">
                          <strong data-start="2320" data-end="2344"
                            >Querying k-NN Search</strong
                          >
                        </h3>
                        <pre
                          class="!overflow-visible"
                          data-start="2345"
                          data-end="2651"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">json</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-json"><span>POST my_vector_index/_search
{
  "size": 5,
  "query": {
    "script_score": {
      "query": { "match_all": {} },
      "script": {
        "source": "cosineSimilarity(params.query_vector, 'text_embedding') + 1.0",
        "params": { "query_vector": [0.1, 0.2, ..., 0.8] }
      }
    }
  }
}
</span></code></div></div></pre>
                        <p data-start="2652" data-end="2671">
                          ✔
                          <strong data-start="2654" data-end="2668"
                            >Advantages</strong
                          >:
                        </p>
                        <ul data-start="2672" data-end="2812">
                          <li data-start="2672" data-end="2731">
                            Can
                            <strong data-start="2678" data-end="2713"
                              >combine keyword &amp; vector search</strong
                            >
                            (hybrid search)
                          </li>
                          <li data-start="2732" data-end="2778">
                            Works well with
                            <strong data-start="2750" data-end="2776"
                              >precomputed embeddings</strong
                            >
                          </li>
                          <li data-start="2779" data-end="2812">
                            Scalable for real-time search
                          </li>
                        </ul>
                        <p data-start="2814" data-end="2834">
                          ❌
                          <strong data-start="2816" data-end="2831"
                            >Limitations</strong
                          >:
                        </p>
                        <ul data-start="2835" data-end="2977">
                          <li data-start="2835" data-end="2897">
                            Still needs
                            <strong data-start="2849" data-end="2875"
                              >precomputed embeddings</strong
                            >
                            for
                            <strong data-start="2880" data-end="2888"
                              >k-NN</strong
                            >
                            search
                          </li>
                          <li data-start="2898" data-end="2977">
                            Not as optimized as FAISS, Pinecone, or Milvus for
                            high-dimensional vectors
                          </li>
                        </ul>
                        <hr data-start="2979" data-end="2982" />
                        <h2 data-start="2984" data-end="3043">
                          <strong data-start="2987" data-end="3043"
                            >4. Hybrid Search (Combining Keyword + Vector
                            Search)</strong
                          >
                        </h2>
                        <p data-start="3044" data-end="3121">
                          Elasticsearch is
                          <strong data-start="3061" data-end="3091"
                            >powerful for hybrid search</strong
                          >, meaning you can
                          <strong data-start="3109" data-end="3120"
                            >combine</strong
                          >:
                        </p>
                        <ul data-start="3122" data-end="3221">
                          <li data-start="3122" data-end="3169">
                            <strong data-start="3124" data-end="3169"
                              >Traditional keyword search (BM25, TF-IDF)</strong
                            >
                          </li>
                          <li data-start="3170" data-end="3221">
                            <strong data-start="3172" data-end="3221"
                              >Semantic search (vector embeddings with
                              k-NN)</strong
                            >
                          </li>
                        </ul>
                        <p data-start="3223" data-end="3258">
                          📌
                          <strong data-start="3226" data-end="3258"
                            >Example: Hybrid Search Query</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="3259"
                          data-end="3756"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">json</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-json"><span>POST my_index/_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "text": "vector database" } },  # BM25 keyword search
        {
          "script_score": {  # Vector similarity search
            "query": { "match_all": {} },
            "script": {
              "source": "cosineSimilarity(params.query_vector, 'text_embedding') + 1.0",
              "params": { "query_vector": [0.1, 0.2, ..., 0.8] }
            }
          }
        }
      ]
    }
  }
}
</span></code></div></div></pre>
                        <p data-start="3757" data-end="3844">
                          This
                          <strong data-start="3762" data-end="3782"
                            >boosts relevance</strong
                          >
                          by combining both
                          <strong data-start="3801" data-end="3843"
                            >traditional search and semantic search</strong
                          >.
                        </p>
                        <hr data-start="3846" data-end="3849" />
                        <h2 data-start="3851" data-end="3918">
                          <strong data-start="3854" data-end="3918"
                            >5. When Should You Use Elasticsearch Instead of a
                            Vector DB?</strong
                          >
                        </h2>
                        <p data-start="3919" data-end="3946">
                          ✅
                          <strong data-start="3921" data-end="3946"
                            >Use Elasticsearch if:</strong
                          >
                        </p>
                        <ul data-start="3947" data-end="4206">
                          <li data-start="3947" data-end="4020">
                            You need a
                            <strong data-start="3960" data-end="3977"
                              >search engine</strong
                            >
                            with
                            <strong data-start="3983" data-end="4003"
                              >full-text search</strong
                            >
                            + vector search.
                          </li>
                          <li data-start="4021" data-end="4076">
                            You want
                            <strong data-start="4032" data-end="4049"
                              >hybrid search</strong
                            >
                            (mixing keyword + vector).
                          </li>
                          <li data-start="4077" data-end="4122">
                            You need
                            <strong data-start="4088" data-end="4121"
                              >real-time search capabilities</strong
                            >.
                          </li>
                          <li data-start="4123" data-end="4206">
                            You
                            <strong data-start="4129" data-end="4138"
                              >don’t</strong
                            >
                            need ultra-high-dimensional vector search (like
                            FAISS or Pinecone).
                          </li>
                        </ul>
                        <p data-start="4208" data-end="4237">
                          ❌
                          <strong data-start="4210" data-end="4237"
                            >Avoid Elasticsearch if:</strong
                          >
                        </p>
                        <ul data-start="4238" data-end="4447">
                          <li data-start="4238" data-end="4327">
                            You are working with
                            <strong data-start="4261" data-end="4301"
                              >millions of high-dimensional vectors</strong
                            >
                            (FAISS/Milvus is better).
                          </li>
                          <li data-start="4328" data-end="4378">
                            You need
                            <strong data-start="4339" data-end="4377"
                              >state-of-the-art similarity search</strong
                            >.
                          </li>
                          <li data-start="4379" data-end="4447">
                            You want a
                            <strong data-start="4392" data-end="4418"
                              >vector-native database</strong
                            >
                            optimized for fast indexing.
                          </li>
                        </ul>
                        <hr data-start="4449" data-end="4452" />
                        <h2 data-start="4454" data-end="4510">
                          <strong data-start="4457" data-end="4510"
                            >Final Verdict: Elasticsearch vs. Vector
                            Databases</strong
                          >
                        </h2>
                        <table data-start="4511" data-end="5153">
                          <thead data-start="4511" data-end="4601">
                            <tr data-start="4511" data-end="4601">
                              <th data-start="4511" data-end="4532">Feature</th>
                              <th data-start="4532" data-end="4555">
                                Elasticsearch
                              </th>
                              <th data-start="4555" data-end="4601">
                                Vector Databases (FAISS, Pinecone, Milvus)
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="4683" data-end="5153">
                            <tr data-start="4683" data-end="4781">
                              <td>
                                <strong data-start="4685" data-end="4700"
                                  >Text Search</strong
                                >
                              </td>
                              <td>✅ Best for keyword-based search</td>
                              <td>
                                ❌ Not designed for traditional text search
                              </td>
                            </tr>
                            <tr data-start="4782" data-end="4877">
                              <td>
                                <strong data-start="4784" data-end="4801"
                                  >Hybrid Search</strong
                                >
                              </td>
                              <td>✅ Supports both BM25 + Vectors</td>
                              <td>❌ Mostly focuses on pure vector search</td>
                            </tr>
                            <tr data-start="4878" data-end="4964">
                              <td>
                                <strong data-start="4880" data-end="4897"
                                  >Vector Search</strong
                                >
                              </td>
                              <td>⚠️ Good but less optimized</td>
                              <td>✅ Highly optimized for ANN search</td>
                            </tr>
                            <tr data-start="4965" data-end="5055">
                              <td>
                                <strong data-start="4967" data-end="4982"
                                  >Scalability</strong
                                >
                              </td>
                              <td>✅ Scales well for real-time apps</td>
                              <td>✅ Scales better for dense vectors</td>
                            </tr>
                            <tr data-start="5056" data-end="5153">
                              <td>
                                <strong data-start="5058" data-end="5073"
                                  >Ease of Use</strong
                                >
                              </td>
                              <td>✅ Easy setup for search applications</td>
                              <td>❌ Requires additional AI model setup</td>
                            </tr>
                          </tbody>
                        </table>
                        <p data-start="5155" data-end="5292">
                          🚀
                          <strong data-start="5158" data-end="5193"
                            >Best Use Case for Elasticsearch</strong
                          >: If you need a
                          <strong data-start="5209" data-end="5264"
                            >search engine that blends keyword and vector
                            search</strong
                          >
                          without relying 100% on AI.
                        </p>
                        <p
                          data-start="5294"
                          data-end="5388"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like a hands-on
                          <strong data-start="5320" data-end="5336"
                            >code example</strong
                          >
                          for setting up vector search with Elasticsearch? 🔥
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-10"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="e3443772-c533-4cd1-bebf-6481e226f26d"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          What is state-of-the-art similarity?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-11"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="76090b48-c610-4818-aaa4-d9304e70d605"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="51">
                          <strong data-start="4" data-end="51"
                            >What Is State-of-the-Art (SOTA) Similarity?</strong
                          >
                        </h3>
                        <p data-start="52" data-end="390">
                          <strong data-start="52" data-end="90"
                            >State-of-the-art (SOTA) similarity</strong
                          >
                          refers to the most
                          <strong data-start="110" data-end="144"
                            >advanced and efficient methods</strong
                          >
                          for measuring how
                          <strong data-start="163" data-end="197"
                            >similar two pieces of data are</strong
                          >, such as text, images, audio, or structured data.
                          These methods use the latest advancements in
                          <strong data-start="293" data-end="351"
                            >machine learning, mathematics, and indexing
                            techniques</strong
                          >
                          to ensure
                          <strong data-start="362" data-end="389"
                            >high accuracy and speed</strong
                          >.
                        </p>
                        <h3 data-start="392" data-end="428">
                          <strong data-start="396" data-end="428"
                            >Why Is Similarity Important?</strong
                          >
                        </h3>
                        <p data-start="429" data-end="492">
                          Many real-world applications rely on
                          <strong data-start="466" data-end="491"
                            >finding similar items</strong
                          >:
                        </p>
                        <ul data-start="493" data-end="853">
                          <li data-start="493" data-end="572">
                            <strong data-start="495" data-end="513"
                              >Search Engines</strong
                            >
                            → Finding documents, products, or images based on a
                            query.
                          </li>
                          <li data-start="573" data-end="648">
                            <strong data-start="575" data-end="601"
                              >Recommendation Systems</strong
                            >
                            → Suggesting movies, music, or shopping items.
                          </li>
                          <li data-start="649" data-end="720">
                            <strong data-start="651" data-end="670"
                              >Fraud Detection</strong
                            >
                            → Identifying patterns in financial transactions.
                          </li>
                          <li data-start="721" data-end="789">
                            <strong data-start="723" data-end="744"
                              >Anomaly Detection</strong
                            >
                            → Finding unusual behavior in cybersecurity.
                          </li>
                          <li data-start="790" data-end="853">
                            <strong data-start="792" data-end="806"
                              >Biometrics</strong
                            >
                            → Face recognition, fingerprint matching, etc.
                          </li>
                        </ul>
                        <hr data-start="855" data-end="858" />
                        <h2 data-start="860" data-end="908">
                          <strong data-start="863" data-end="908"
                            >Key Approaches for SOTA Similarity Search</strong
                          >
                        </h2>
                        <p data-start="909" data-end="954">
                          The most advanced similarity methods rely on:
                        </p>
                        <ol data-start="955" data-end="1117">
                          <li data-start="955" data-end="992">
                            <strong data-start="958" data-end="978"
                              >Distance Metrics</strong
                            >
                            (Traditional)
                          </li>
                          <li data-start="993" data-end="1046">
                            <strong data-start="996" data-end="1023"
                              >Machine Learning Models</strong
                            >
                            (AI-driven embeddings)
                          </li>
                          <li data-start="1047" data-end="1117">
                            <strong data-start="1050" data-end="1100"
                              >Approximate Nearest Neighbors (ANN)
                              Algorithms</strong
                            >
                            (Fast retrieval)
                          </li>
                        </ol>
                        <p data-start="1119" data-end="1141">
                          Let’s break each down.
                        </p>
                        <hr data-start="1143" data-end="1146" />
                        <h2 data-start="1148" data-end="1197">
                          <strong data-start="1151" data-end="1197"
                            >1. Distance Metrics (Mathematical Methods)</strong
                          >
                        </h2>
                        <p data-start="1198" data-end="1289">
                          Traditional
                          <strong data-start="1210" data-end="1239"
                            >distance-based similarity</strong
                          >
                          methods compare data points in a numerical space.
                        </p>
                        <h3 data-start="1291" data-end="1323">
                          <strong data-start="1295" data-end="1323"
                            >Common Distance Metrics:</strong
                          >
                        </h3>
                        <table data-start="1324" data-end="1756">
                          <thead data-start="1324" data-end="1355">
                            <tr data-start="1324" data-end="1355">
                              <th data-start="1324" data-end="1333">Metric</th>
                              <th data-start="1333" data-end="1343">Formula</th>
                              <th data-start="1343" data-end="1355">
                                Best For
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="1388" data-end="1756">
                            <tr data-start="1388" data-end="1482">
                              <td>
                                <strong data-start="1390" data-end="1411"
                                  >Cosine Similarity</strong
                                >
                              </td>
                              <td>( \cos(\theta) = \frac{A \cdot B}{</td>
                              <td></td>
                            </tr>
                            <tr data-start="1483" data-end="1575">
                              <td>
                                <strong data-start="1485" data-end="1507"
                                  >Euclidean Distance</strong
                                >
                              </td>
                              <td>
                                <span class="katex"
                                  ><span class="katex-mathml"
                                    ><math
                                      xmlns="http://www.w3.org/1998/Math/MathML"
                                      ><semantics
                                        ><mrow
                                          ><mi>d</mi><mo stretchy="false">(</mo
                                          ><mi>A</mi><mo separator="true">,</mo
                                          ><mi>B</mi><mo stretchy="false">)</mo
                                          ><mo>=</mo
                                          ><msqrt
                                            ><mrow
                                              ><mo>∑</mo
                                              ><mo stretchy="false">(</mo
                                              ><msub><mi>A</mi><mi>i</mi></msub
                                              ><mo>−</mo
                                              ><msub><mi>B</mi><mi>i</mi></msub
                                              ><msup
                                                ><mo stretchy="false">)</mo
                                                ><mn>2</mn></msup
                                              ></mrow
                                            ></msqrt
                                          ></mrow
                                        ><annotation
                                          encoding="application/x-tex"
                                          >d(A, B) = \sqrt{\sum (A_i -
                                          B_i)^2}</annotation
                                        ></semantics
                                      ></math
                                    ></span
                                  ><span class="katex-html" aria-hidden="true"
                                    ><span class="base"
                                      ><span
                                        class="strut"
                                        style="
                                          height: 1em;
                                          vertical-align: -0.25em;
                                        "
                                      ></span
                                      ><span class="mord mathnormal">d</span
                                      ><span class="mopen">(</span
                                      ><span class="mord mathnormal">A</span
                                      ><span class="mpunct">,</span
                                      ><span
                                        class="mspace"
                                        style="margin-right: 0.1667em"
                                      ></span
                                      ><span
                                        class="mord mathnormal"
                                        style="margin-right: 0.05017em"
                                        >B</span
                                      ><span class="mclose">)</span
                                      ><span
                                        class="mspace"
                                        style="margin-right: 0.2778em"
                                      ></span
                                      ><span class="mrel">=</span
                                      ><span
                                        class="mspace"
                                        style="margin-right: 0.2778em"
                                      ></span></span
                                    ><span class="base"
                                      ><span
                                        class="strut"
                                        style="
                                          height: 1.24em;
                                          vertical-align: -0.305em;
                                        "
                                      ></span
                                      ><span class="mord sqrt"
                                        ><span class="vlist-t vlist-t2"
                                          ><span class="vlist-r"
                                            ><span
                                              class="vlist"
                                              style="height: 0.935em"
                                              ><span
                                                class="svg-align"
                                                style="top: -3.2em"
                                                ><span
                                                  class="pstrut"
                                                  style="height: 3.2em"
                                                ></span
                                                ><span
                                                  class="mord"
                                                  style="padding-left: 1em"
                                                  ><span
                                                    class="mop op-symbol small-op"
                                                    style="
                                                      position: relative;
                                                      top: 0em;
                                                    "
                                                    >∑</span
                                                  ><span class="mopen">(</span
                                                  ><span class="mord"
                                                    ><span
                                                      class="mord mathnormal"
                                                      >A</span
                                                    ><span class="msupsub"
                                                      ><span
                                                        class="vlist-t vlist-t2"
                                                        ><span class="vlist-r"
                                                          ><span
                                                            class="vlist"
                                                            style="
                                                              height: 0.3117em;
                                                            "
                                                            ><span
                                                              style="
                                                                top: -2.55em;
                                                                margin-left: 0em;
                                                                margin-right: 0.05em;
                                                              "
                                                              ><span
                                                                class="pstrut"
                                                                style="
                                                                  height: 2.7em;
                                                                "
                                                              ></span
                                                              ><span
                                                                class="sizing reset-size6 size3 mtight"
                                                                ><span
                                                                  class="mord mathnormal mtight"
                                                                  >i</span
                                                                ></span
                                                              ></span
                                                            ></span
                                                          ><span class="vlist-s"
                                                            >​</span
                                                          ></span
                                                        ><span class="vlist-r"
                                                          ><span
                                                            class="vlist"
                                                            style="
                                                              height: 0.15em;
                                                            "
                                                            ><span></span></span></span></span></span></span
                                                  ><span
                                                    class="mspace"
                                                    style="
                                                      margin-right: 0.2222em;
                                                    "
                                                  ></span
                                                  ><span class="mbin">−</span
                                                  ><span
                                                    class="mspace"
                                                    style="
                                                      margin-right: 0.2222em;
                                                    "
                                                  ></span
                                                  ><span class="mord"
                                                    ><span
                                                      class="mord mathnormal"
                                                      style="
                                                        margin-right: 0.05017em;
                                                      "
                                                      >B</span
                                                    ><span class="msupsub"
                                                      ><span
                                                        class="vlist-t vlist-t2"
                                                        ><span class="vlist-r"
                                                          ><span
                                                            class="vlist"
                                                            style="
                                                              height: 0.3117em;
                                                            "
                                                            ><span
                                                              style="
                                                                top: -2.55em;
                                                                margin-left: -0.0502em;
                                                                margin-right: 0.05em;
                                                              "
                                                              ><span
                                                                class="pstrut"
                                                                style="
                                                                  height: 2.7em;
                                                                "
                                                              ></span
                                                              ><span
                                                                class="sizing reset-size6 size3 mtight"
                                                                ><span
                                                                  class="mord mathnormal mtight"
                                                                  >i</span
                                                                ></span
                                                              ></span
                                                            ></span
                                                          ><span class="vlist-s"
                                                            >​</span
                                                          ></span
                                                        ><span class="vlist-r"
                                                          ><span
                                                            class="vlist"
                                                            style="
                                                              height: 0.15em;
                                                            "
                                                            ><span></span></span></span></span></span></span
                                                  ><span class="mclose"
                                                    ><span class="mclose"
                                                      >)</span
                                                    ><span class="msupsub"
                                                      ><span class="vlist-t"
                                                        ><span class="vlist-r"
                                                          ><span
                                                            class="vlist"
                                                            style="
                                                              height: 0.7401em;
                                                            "
                                                            ><span
                                                              style="
                                                                top: -2.989em;
                                                                margin-right: 0.05em;
                                                              "
                                                              ><span
                                                                class="pstrut"
                                                                style="
                                                                  height: 2.7em;
                                                                "
                                                              ></span
                                                              ><span
                                                                class="sizing reset-size6 size3 mtight"
                                                                ><span
                                                                  class="mord mtight"
                                                                  >2</span
                                                                ></span
                                                              ></span
                                                            ></span
                                                          ></span
                                                        ></span
                                                      ></span
                                                    ></span
                                                  ></span
                                                ></span
                                              ><span style="top: -2.895em"
                                                ><span
                                                  class="pstrut"
                                                  style="height: 3.2em"
                                                ></span
                                                ><span
                                                  class="hide-tail"
                                                  style="
                                                    min-width: 1.02em;
                                                    height: 1.28em;
                                                  "
                                                ></span></span></span
                                            ><span class="vlist-s"
                                              >​</span
                                            ></span
                                          ><span class="vlist-r"
                                            ><span
                                              class="vlist"
                                              style="height: 0.305em"
                                              ><span></span></span></span></span></span></span></span
                                ></span>
                              </td>
                              <td>
                                <strong data-start="1554" data-end="1573"
                                  >Images, 3D data</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="1576" data-end="1659">
                              <td>
                                <strong data-start="1578" data-end="1600"
                                  >Manhattan Distance</strong
                                >
                              </td>
                              <td>( d(A, B) = \sum</td>
                              <td>A_i - B_i</td>
                            </tr>
                            <tr data-start="1660" data-end="1756">
                              <td>
                                <strong data-start="1662" data-end="1684"
                                  >Jaccard Similarity</strong
                                >
                              </td>
                              <td>( J(A, B) = \frac{</td>
                              <td>A \cap B</td>
                            </tr>
                          </tbody>
                        </table>
                        <p data-start="1758" data-end="1801">
                          📌
                          <strong data-start="1761" data-end="1801"
                            >Example: Cosine Similarity in Python</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1802"
                          data-end="2032"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

vec1 = np.array([[1, 2, 3]])
vec2 = np.array([[2, 3, 4]])

similarity = cosine_similarity(vec1, vec2)
print(similarity)  # Output: [[0.9925]]
</span></code></div></div></pre>
                        <p data-start="2033" data-end="2173">
                          ✅
                          <strong data-start="2035" data-end="2043">Pros</strong
                          >: Simple, interpretable, works well for small
                          datasets.<br data-start="2098" data-end="2101" />❌
                          <strong data-start="2103" data-end="2111">Cons</strong
                          >:
                          <strong data-start="2113" data-end="2144"
                            >Slow for large-scale search</strong
                          >
                          (e.g., millions of items).
                        </p>
                        <hr data-start="2175" data-end="2178" />
                        <h2 data-start="2180" data-end="2239">
                          <strong data-start="2183" data-end="2239"
                            >2. Machine Learning-Based Similarity (AI
                            Embeddings)</strong
                          >
                        </h2>
                        <p data-start="2240" data-end="2376">
                          Instead of manually computing distances,
                          <strong data-start="2281" data-end="2294"
                            >AI models</strong
                          >
                          can
                          <strong data-start="2299" data-end="2331"
                            >learn better representations</strong
                          >
                          (embeddings) to improve similarity accuracy.
                        </p>
                        <h3 data-start="2378" data-end="2419">
                          <strong data-start="2382" data-end="2419"
                            >State-of-the-Art Embedding Models</strong
                          >
                        </h3>
                        <table data-start="2420" data-end="2716">
                          <thead data-start="2420" data-end="2447">
                            <tr data-start="2420" data-end="2447">
                              <th data-start="2420" data-end="2428">Model</th>
                              <th data-start="2428" data-end="2435">Type</th>
                              <th data-start="2435" data-end="2447">
                                Best For
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="2480" data-end="2716">
                            <tr data-start="2480" data-end="2531">
                              <td>
                                <strong data-start="2482" data-end="2498"
                                  >BERT / SBERT</strong
                                >
                              </td>
                              <td>Text</td>
                              <td>NLP, Chatbots, Search</td>
                            </tr>
                            <tr data-start="2532" data-end="2596">
                              <td>
                                <strong data-start="2534" data-end="2549"
                                  >OpenAI CLIP</strong
                                >
                              </td>
                              <td>Image &amp; Text</td>
                              <td>Image retrieval, Captioning</td>
                            </tr>
                            <tr data-start="2597" data-end="2657">
                              <td>
                                <strong data-start="2599" data-end="2609"
                                  >DINOv2</strong
                                >
                              </td>
                              <td>Vision</td>
                              <td>Object detection, Face Recognition</td>
                            </tr>
                            <tr data-start="2658" data-end="2716">
                              <td>
                                <strong data-start="2660" data-end="2671"
                                  >Whisper</strong
                                >
                              </td>
                              <td>Audio</td>
                              <td>Speech-to-text, Speaker Matching</td>
                            </tr>
                          </tbody>
                        </table>
                        <p data-start="2718" data-end="2794">
                          📌
                          <strong data-start="2721" data-end="2794"
                            >Example: Using OpenAI’s
                            <code data-start="2747" data-end="2771"
                              >text-embedding-ada-002</code
                            >
                            Model for Similarity</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="2795"
                          data-end="3092"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from openai import OpenAI

client = OpenAI(api_key="your_api_key")

query = "best restaurants in New York"
response = client.embeddings.create(model="text-embedding-ada-002", input=query)
embedding = response['data'][0]['embedding']

print(embedding[:5])  # Prints first 5 dimensions
</span></code></div></div></pre>
                        <p data-start="3093" data-end="3273">
                          ✅
                          <strong data-start="3095" data-end="3103">Pros</strong
                          >: Captures
                          <strong data-start="3114" data-end="3134"
                            >semantic meaning</strong
                          >, robust to noise,
                          <strong data-start="3153" data-end="3182"
                            >handles unstructured data</strong
                          >.<br data-start="3183" data-end="3186" />❌
                          <strong data-start="3188" data-end="3196">Cons</strong
                          >: Requires
                          <strong data-start="3207" data-end="3223"
                            >pre-training</strong
                          >
                          and
                          <strong data-start="3228" data-end="3246"
                            >vector storage</strong
                          >
                          for efficient retrieval.
                        </p>
                        <hr data-start="3275" data-end="3278" />
                        <h2 data-start="3280" data-end="3342">
                          <strong data-start="3283" data-end="3342"
                            >3. Approximate Nearest Neighbors (ANN) - Fast
                            Retrieval</strong
                          >
                        </h2>
                        <p data-start="3343" data-end="3549">
                          Traditional similarity search is
                          <strong data-start="3376" data-end="3403"
                            >slow for large datasets</strong
                          >
                          (millions of vectors). To solve this,
                          <strong data-start="3442" data-end="3481"
                            >ANN (Approximate Nearest Neighbors)</strong
                          >
                          algorithms
                          <strong data-start="3493" data-end="3512"
                            >speed up search</strong
                          >
                          by indexing vectors
                          <strong data-start="3533" data-end="3548"
                            >efficiently</strong
                          >.
                        </p>
                        <h3 data-start="3551" data-end="3600">
                          <strong data-start="3555" data-end="3600"
                            >Best ANN Algorithms for Similarity Search</strong
                          >
                        </h3>
                        <table data-start="3601" data-end="3973">
                          <thead data-start="3601" data-end="3641">
                            <tr data-start="3601" data-end="3641">
                              <th data-start="3601" data-end="3613">
                                Algorithm
                              </th>
                              <th data-start="3613" data-end="3623">Library</th>
                              <th data-start="3623" data-end="3641">
                                Best Use Cases
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="3682" data-end="3973">
                            <tr data-start="3682" data-end="3782">
                              <td>
                                <strong data-start="3684" data-end="3729"
                                  >HNSW (Hierarchical Navigable Small
                                  World)</strong
                                >
                              </td>
                              <td>FAISS, Milvus</td>
                              <td>High-speed large-scale retrieval</td>
                            </tr>
                            <tr data-start="3783" data-end="3871">
                              <td>
                                <strong data-start="3785" data-end="3814"
                                  >IVF (Inverted File Index)</strong
                                >
                              </td>
                              <td>FAISS</td>
                              <td>
                                Efficient clustering for billions of vectors
                              </td>
                            </tr>
                            <tr data-start="3872" data-end="3973">
                              <td>
                                <strong data-start="3874" data-end="3910"
                                  >LSH (Locality-Sensitive Hashing)</strong
                                >
                              </td>
                              <td>ScaNN, Annoy</td>
                              <td>
                                Quick filtering for nearest neighbor search
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <p data-start="3975" data-end="4025">
                          📌
                          <strong data-start="3978" data-end="4025"
                            >Example: Using FAISS for Fast Vector Search</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="4026"
                          data-end="4427"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import faiss
import numpy as np

# Create random 128D vectors
vector_data = np.random.rand(10000, 128).astype('float32')

# Create an index
index = faiss.IndexFlatL2(128)
index.add(vector_data)

# Query the 5 closest vectors
query_vector = np.random.rand(1, 128).astype('float32')
_, result_indices = index.search(query_vector, 5)

print(result_indices)  # Returns closest vector indices
</span></code></div></div></pre>
                        <p data-start="4428" data-end="4569">
                          ✅
                          <strong data-start="4430" data-end="4438">Pros</strong
                          >:
                          <strong data-start="4440" data-end="4454"
                            >Super fast</strong
                          >, scalable for
                          <strong data-start="4469" data-end="4492"
                            >millions of vectors</strong
                          >.<br data-start="4493" data-end="4496" />❌
                          <strong data-start="4498" data-end="4506">Cons</strong
                          >: Needs
                          <strong data-start="4514" data-end="4531"
                            >preprocessing</strong
                          >
                          and
                          <strong data-start="4536" data-end="4566"
                            >proper indexing strategies</strong
                          >.
                        </p>
                        <hr data-start="4571" data-end="4574" />
                        <h2 data-start="4576" data-end="4633">
                          <strong data-start="4579" data-end="4633"
                            >Hybrid Similarity Approaches (Best of Both
                            Worlds)</strong
                          >
                        </h2>
                        <p data-start="4634" data-end="4703">
                          The
                          <strong data-start="4638" data-end="4670"
                            >best SOTA similarity methods</strong
                          >
                          <strong data-start="4671" data-end="4702"
                            >combine multiple techniques</strong
                          >:
                        </p>
                        <ul data-start="4704" data-end="4856">
                          <li data-start="4704" data-end="4753">
                            <strong data-start="4706" data-end="4753"
                              >Keyword Search (BM25) + Vector Search
                              (ANN)</strong
                            >
                          </li>
                          <li data-start="4754" data-end="4801">
                            <strong data-start="4756" data-end="4801"
                              >Rule-Based Matching + AI-Based Similarity</strong
                            >
                          </li>
                          <li data-start="4802" data-end="4856">
                            <strong data-start="4804" data-end="4856"
                              >Traditional Distance Metrics + Neural
                              Embeddings</strong
                            >
                          </li>
                        </ul>
                        <p data-start="4858" data-end="4907">
                          📌
                          <strong data-start="4861" data-end="4907"
                            >Example: Hybrid Search Using Elasticsearch</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="4908"
                          data-end="5412"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">json</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-json"><span>POST my_index/_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "text": "deep learning research" } },  # BM25 keyword search
        {
          "script_score": {  # Vector similarity search
            "query": { "match_all": {} },
            "script": {
              "source": "cosineSimilarity(params.query_vector, 'text_embedding') + 1.0",
              "params": { "query_vector": [0.1, 0.2, ..., 0.8] }
            }
          }
        }
      ]
    }
  }
}
</span></code></div></div></pre>
                        <p data-start="5413" data-end="5578">
                          ✅
                          <strong data-start="5415" data-end="5423">Pros</strong
                          >:
                          <strong data-start="5425" data-end="5452"
                            >Best retrieval accuracy</strong
                          >, works well in
                          <strong data-start="5468" data-end="5495"
                            >real-world applications</strong
                          >.<br data-start="5496" data-end="5499" />❌
                          <strong data-start="5501" data-end="5509">Cons</strong
                          >: More
                          <strong data-start="5516" data-end="5545"
                            >computationally intensive</strong
                          >, needs
                          <strong data-start="5553" data-end="5575"
                            >efficient indexing</strong
                          >.
                        </p>
                        <hr data-start="5580" data-end="5583" />
                        <h2 data-start="5585" data-end="5626">
                          <strong data-start="5588" data-end="5626"
                            >Final Thoughts: What’s Truly SOTA?</strong
                          >
                        </h2>
                        <p data-start="5627" data-end="5722">
                          🚀 The
                          <strong data-start="5634" data-end="5660"
                            >best similarity method</strong
                          >
                          depends on
                          <strong data-start="5672" data-end="5721"
                            >your dataset, speed needs, and accuracy
                            goals</strong
                          >:
                        </p>
                        <p data-start="5724" data-end="6065">
                          ✔
                          <strong data-start="5726" data-end="5744"
                            >Small datasets</strong
                          >
                          → Use
                          <strong data-start="5751" data-end="5794"
                            >Cosine Similarity or Jaccard Similarity</strong
                          ><br data-start="5794" data-end="5797" />✔
                          <strong data-start="5799" data-end="5821"
                            >Large-scale search</strong
                          >
                          → Use
                          <strong data-start="5828" data-end="5859"
                            >FAISS (HNSW, IVF) or Milvus</strong
                          ><br data-start="5859" data-end="5862" />✔
                          <strong data-start="5864" data-end="5879"
                            >Text Search</strong
                          >
                          → Use
                          <strong data-start="5886" data-end="5935"
                            >SBERT, BM25, or hybrid search (BM25 +
                            Vector)</strong
                          ><br data-start="5935" data-end="5938" />✔
                          <strong data-start="5940" data-end="5956"
                            >Image Search</strong
                          >
                          → Use
                          <strong data-start="5963" data-end="5990"
                            >CLIP embeddings + FAISS</strong
                          ><br data-start="5990" data-end="5993" />✔
                          <strong data-start="5995" data-end="6021"
                            >Real-Time Applications</strong
                          >
                          → Use
                          <strong data-start="6028" data-end="6063"
                            >Elasticsearch for Hybrid Search</strong
                          >
                        </p>
                        <p
                          data-start="6067"
                          data-end="6163"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like a
                          <strong data-start="6084" data-end="6124"
                            >hands-on demo or code implementation</strong
                          >
                          for a specific similarity use case? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-12"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="53133654-9fca-4c6a-926e-62aab8f279aa"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          How many dimensions is suitable for elastic-search?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-13"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="16d2f3c1-1d9e-447a-b367-ff6f5533bc7e"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="59">
                          <strong data-start="4" data-end="59"
                            >How Many Dimensions Are Suitable for
                            Elasticsearch?</strong
                          >
                        </h3>
                        <p data-start="60" data-end="238">
                          Elasticsearch supports
                          <strong data-start="83" data-end="106"
                            >dense vector fields</strong
                          >
                          for similarity search, but the optimal number of
                          dimensions depends on
                          <strong data-start="178" data-end="237"
                            >your dataset size, search speed, and memory
                            constraints</strong
                          >.
                        </p>
                        <h3 data-start="240" data-end="268">
                          <strong data-start="244" data-end="268"
                            >Official Limitations</strong
                          >
                        </h3>
                        <p data-start="269" data-end="501">
                          ✔
                          <strong data-start="271" data-end="290"
                            >Max Dimensions:</strong
                          >
                          <strong data-start="291" data-end="339"
                            >Elasticsearch supports up to 1024
                            dimensions</strong
                          >.<br data-start="340" data-end="343" />✔
                          <strong data-start="345" data-end="372"
                            >Recommended Dimensions:</strong
                          >
                          <strong data-start="373" data-end="394"
                            >50–768 dimensions</strong
                          >
                          (for efficiency).<br
                            data-start="412"
                            data-end="415"
                          />✔
                          <strong data-start="417" data-end="435"
                            >Best Practice:</strong
                          >
                          Keep vectors
                          <strong data-start="449" data-end="473"
                            >under 300 dimensions</strong
                          >
                          for
                          <strong data-start="478" data-end="500"
                            >better performance</strong
                          >.
                        </p>
                        <hr data-start="503" data-end="506" />
                        <h2 data-start="508" data-end="554">
                          <strong data-start="511" data-end="554"
                            >Choosing the Right Number of Dimensions</strong
                          >
                        </h2>
                        <table data-start="555" data-end="1073">
                          <thead data-start="555" data-end="620">
                            <tr data-start="555" data-end="620">
                              <th data-start="555" data-end="576">
                                <strong data-start="557" data-end="575"
                                  >Dimensionality</strong
                                >
                              </th>
                              <th data-start="576" data-end="591">
                                <strong data-start="578" data-end="590"
                                  >Use Case</strong
                                >
                              </th>
                              <th data-start="591" data-end="620">
                                <strong data-start="593" data-end="618"
                                  >Performance Trade-off</strong
                                >
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="672" data-end="1073">
                            <tr data-start="672" data-end="757">
                              <td>
                                <strong data-start="674" data-end="683"
                                  >10–50</strong
                                >
                              </td>
                              <td>Simple keyword embedding (TF-IDF, LSH)</td>
                              <td>🔹 Fastest, but low accuracy</td>
                            </tr>
                            <tr data-start="758" data-end="866">
                              <td>
                                <strong data-start="760" data-end="771"
                                  >100–300</strong
                                >
                              </td>
                              <td>
                                General-purpose NLP (SBERT, sentence embeddings)
                              </td>
                              <td>✅ Best for most Elasticsearch workloads</td>
                            </tr>
                            <tr data-start="867" data-end="959">
                              <td>
                                <strong data-start="869" data-end="880"
                                  >512–768</strong
                                >
                              </td>
                              <td>Transformer-based embeddings (BERT, CLIP)</td>
                              <td>⚠️ Requires more RAM &amp; compute</td>
                            </tr>
                            <tr data-start="960" data-end="1073">
                              <td>
                                <strong data-start="962" data-end="971"
                                  >1024+</strong
                                >
                              </td>
                              <td>Specialized AI models (OpenAI embeddings)</td>
                              <td>
                                ❌ Not recommended for Elasticsearch (FAISS is
                                better)
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <hr data-start="1075" data-end="1078" />
                        <h2 data-start="1080" data-end="1123">
                          <strong data-start="1083" data-end="1123"
                            >Performance vs. Dimension Trade-offs</strong
                          >
                        </h2>
                        <ul data-start="1124" data-end="1367">
                          <li data-start="1124" data-end="1194">
                            <strong data-start="1126" data-end="1147"
                              >Higher dimensions</strong
                            >
                            improve
                            <strong data-start="1156" data-end="1168"
                              >accuracy</strong
                            >
                            but
                            <strong data-start="1173" data-end="1193"
                              >slow down search</strong
                            >.
                          </li>
                          <li data-start="1195" data-end="1270">
                            <strong data-start="1197" data-end="1217"
                              >Lower dimensions</strong
                            >
                            improve
                            <strong data-start="1226" data-end="1235"
                              >speed</strong
                            >
                            but
                            <strong data-start="1240" data-end="1269"
                              >reduce similarity quality</strong
                            >.
                          </li>
                          <li data-start="1271" data-end="1367">
                            <strong data-start="1273" data-end="1341"
                              >Elasticsearch is not optimized for
                              ultra-high-dimensional search</strong
                            >
                            (like FAISS or Pinecone).
                          </li>
                        </ul>
                        <hr data-start="1369" data-end="1372" />
                        <h2 data-start="1374" data-end="1427">
                          <strong data-start="1377" data-end="1427"
                            >Best Practices for Elasticsearch Vector
                            Search</strong
                          >
                        </h2>
                        <p data-start="1428" data-end="1661">
                          ✔
                          <strong data-start="1430" data-end="1458"
                            >Try reducing vector size</strong
                          >
                          (e.g., use PCA to compress high-dimensional
                          embeddings).<br data-start="1515" data-end="1518" />✔
                          <strong data-start="1520" data-end="1557"
                            >Use efficient indexing techniques</strong
                          >
                          (like HNSW for ANN search).<br
                            data-start="1585"
                            data-end="1588"
                          />✔
                          <strong data-start="1590" data-end="1639"
                            >Combine keyword search (BM25) + vector
                            search</strong
                          >
                          for better results.
                        </p>
                        <h3 data-start="1663" data-end="1716">
                          <strong data-start="1667" data-end="1716"
                            >Example: Using a 300D Vector in
                            Elasticsearch</strong
                          >
                        </h3>
                        <pre
                          class="!overflow-visible"
                          data-start="1717"
                          data-end="1877"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">json</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-json"><span>PUT my_index
{
  "mappings": {
    "properties": {
      "text_embedding": {
        "type": "dense_vector",
        "dims": 300
      }
    }
  }
}
</span></code></div></div></pre>
                        <p
                          data-start="1879"
                          data-end="1975"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like help
                          <strong data-start="1899" data-end="1945"
                            >optimizing Elasticsearch for vector search</strong
                          >
                          in your specific use case? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-14"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="04d4766f-5925-4aa7-8db4-58f08fddb810"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          I didn't understand the Choosing the right number of
                          dimensions section
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-15"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="338e0073-08c0-4418-97ea-bcbbf58c2000"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="65">
                          <strong data-start="4" data-end="65"
                            >Choosing the Right Number of Dimensions for
                            Elasticsearch</strong
                          >
                        </h3>
                        <p data-start="66" data-end="276">
                          The
                          <strong data-start="70" data-end="94"
                            >number of dimensions</strong
                          >
                          in a vector represents how much
                          <strong data-start="127" data-end="142"
                            >information</strong
                          >
                          is stored about the data. However,
                          <strong data-start="178" data-end="199"
                            >higher dimensions</strong
                          >
                          can slow down search performance, so finding the right
                          balance is important.
                        </p>
                        <p data-start="278" data-end="327">
                          Let’s break it down based on different use cases:
                        </p>
                        <table data-start="329" data-end="986">
                          <thead data-start="329" data-end="397">
                            <tr data-start="329" data-end="397">
                              <th data-start="329" data-end="356">
                                <strong data-start="331" data-end="355"
                                  >Number of Dimensions</strong
                                >
                              </th>
                              <th data-start="356" data-end="378">
                                <strong data-start="358" data-end="377"
                                  >When to Use It?</strong
                                >
                              </th>
                              <th data-start="378" data-end="397">
                                <strong data-start="380" data-end="395"
                                  >Pros &amp; Cons</strong
                                >
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="463" data-end="986">
                            <tr data-start="463" data-end="576">
                              <td>
                                <strong data-start="465" data-end="474"
                                  >10–50</strong
                                >
                              </td>
                              <td>
                                <strong data-start="477" data-end="494"
                                  >Simple search</strong
                                >
                                (e.g., TF-IDF, keyword embeddings)
                              </td>
                              <td>
                                ✅
                                <strong data-start="534" data-end="547"
                                  >Very fast</strong
                                >
                                search, ❌
                                <strong data-start="558" data-end="574"
                                  >Low accuracy</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="577" data-end="696">
                              <td>
                                <strong data-start="579" data-end="590"
                                  >100–300</strong
                                >
                              </td>
                              <td>
                                <strong data-start="593" data-end="618"
                                  >Most NLP applications</strong
                                >
                                (e.g., SBERT, sentence embeddings)
                              </td>
                              <td>
                                ✅
                                <strong data-start="658" data-end="694"
                                  >Good balance of speed &amp; accuracy</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="697" data-end="828">
                              <td>
                                <strong data-start="699" data-end="710"
                                  >512–768</strong
                                >
                              </td>
                              <td>
                                <strong data-start="713" data-end="735"
                                  >Advanced AI models</strong
                                >
                                (e.g., BERT, OpenAI embeddings)
                              </td>
                              <td>
                                ✅
                                <strong data-start="772" data-end="789"
                                  >High accuracy</strong
                                >, ❌
                                <strong data-start="793" data-end="826"
                                  >Slower &amp; requires more memory</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="829" data-end="986">
                              <td>
                                <strong data-start="831" data-end="840"
                                  >1024+</strong
                                >
                              </td>
                              <td>
                                <strong data-start="843" data-end="875"
                                  >Complex deep learning models</strong
                                >
                                (e.g., GPT, specialized AI search)
                              </td>
                              <td>
                                ❌
                                <strong data-start="915" data-end="952"
                                  >Not recommended for Elasticsearch</strong
                                >
                                (better to use FAISS or Milvus)
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <hr data-start="988" data-end="991" />
                        <h3 data-start="993" data-end="1014">
                          <strong data-start="997" data-end="1014"
                            >Key Takeaways</strong
                          >
                        </h3>
                        <ul data-start="1015" data-end="1407">
                          <li data-start="1015" data-end="1111">
                            <strong data-start="1017" data-end="1045"
                              >Lower dimensions (10–50)</strong
                            >
                            →
                            <strong data-start="1048" data-end="1076"
                              >Faster but less accurate</strong
                            >
                            (good for keyword-based search).
                          </li>
                          <li data-start="1112" data-end="1215">
                            <strong data-start="1114" data-end="1148"
                              >Mid-range dimensions (100–300)</strong
                            >
                            →
                            <strong data-start="1151" data-end="1177"
                              >Best for Elasticsearch</strong
                            >
                            (good balance of speed &amp; accuracy).
                          </li>
                          <li data-start="1216" data-end="1312">
                            <strong data-start="1218" data-end="1249"
                              >Higher dimensions (512–768)</strong
                            >
                            →
                            <strong data-start="1252" data-end="1280"
                              >More accurate but slower</strong
                            >
                            (best for AI-powered search).
                          </li>
                          <li data-start="1313" data-end="1407">
                            <strong data-start="1315" data-end="1339"
                              >Over 1024 dimensions</strong
                            >
                            →
                            <strong data-start="1342" data-end="1376"
                              >Not suitable for Elasticsearch</strong
                            >
                            (FAISS or Pinecone is better).
                          </li>
                        </ul>
                        <p data-start="1409" data-end="1507">
                          📌
                          <strong data-start="1412" data-end="1504"
                            >For Elasticsearch, try to keep your vector
                            dimensions below 300 for optimal
                            performance.</strong
                          >
                          🚀
                        </p>
                        <p
                          data-start="1509"
                          data-end="1594"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like help choosing the
                          <strong data-start="1542" data-end="1567"
                            >right embedding model</strong
                          >
                          based on your use case? 😊
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-16"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="d808c4c7-2622-4cce-9938-757bdc42cb3f"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          Which way do you propose over elastic-search for each
                          dimension-range
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-17"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="ab8389e1-42d7-45b7-83fe-e9cc5a62995e"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="69">
                          <strong data-start="4" data-end="67"
                            >Best Alternatives to Elasticsearch for Each
                            Dimension Range</strong
                          >
                        </h3>
                        <p data-start="70" data-end="200">
                          If Elasticsearch
                          <strong data-start="87" data-end="112"
                            >isn’t the best option</strong
                          >
                          for your use case, here’s what I recommend based on
                          vector dimensions and search needs:
                        </p>
                        <table data-start="202" data-end="1258">
                          <thead data-start="202" data-end="286">
                            <tr data-start="202" data-end="286">
                              <th data-start="202" data-end="224">
                                <strong data-start="204" data-end="223"
                                  >Dimension Range</strong
                                >
                              </th>
                              <th data-start="224" data-end="244">
                                <strong data-start="226" data-end="243"
                                  >Best Use Case</strong
                                >
                              </th>
                              <th data-start="244" data-end="274">
                                <strong data-start="246" data-end="273"
                                  >Recommended Alternative</strong
                                >
                              </th>
                              <th data-start="274" data-end="286">
                                <strong data-start="276" data-end="284"
                                  >Why?</strong
                                >
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="375" data-end="1258">
                            <tr data-start="375" data-end="577">
                              <td>
                                <strong data-start="377" data-end="386"
                                  >10–50</strong
                                >
                              </td>
                              <td>
                                <strong data-start="389" data-end="436"
                                  >Simple search (TF-IDF, keyword search,
                                  LSH)</strong
                                >
                              </td>
                              <td>
                                ✅
                                <strong data-start="441" data-end="474"
                                  >Elasticsearch (BM25 + TF-IDF)</strong
                                >
                              </td>
                              <td>
                                Elasticsearch is
                                <strong data-start="494" data-end="528"
                                  >great for low-dimensional data</strong
                                >, since keyword search dominates in this range.
                              </td>
                            </tr>
                            <tr data-start="578" data-end="810">
                              <td>
                                <strong data-start="580" data-end="591"
                                  >100–300</strong
                                >
                              </td>
                              <td>
                                <strong data-start="594" data-end="651"
                                  >General NLP (SBERT, small AI models, text
                                  embeddings)</strong
                                >
                              </td>
                              <td>
                                ✅
                                <strong data-start="656" data-end="688"
                                  >Elasticsearch (dense_vector)</strong
                                >
                                OR
                                <strong data-start="692" data-end="708"
                                  >FAISS (HNSW)</strong
                                >
                              </td>
                              <td>
                                Elasticsearch
                                <strong data-start="725" data-end="744"
                                  >works well here</strong
                                >, but
                                <strong data-start="750" data-end="759"
                                  >FAISS</strong
                                >
                                is better if you need faster large-scale search.
                              </td>
                            </tr>
                            <tr data-start="811" data-end="1036">
                              <td>
                                <strong data-start="813" data-end="824"
                                  >512–768</strong
                                >
                              </td>
                              <td>
                                <strong data-start="827" data-end="887"
                                  >Deep learning models (BERT, CLIP, sentence
                                  transformers)</strong
                                >
                              </td>
                              <td>
                                ✅
                                <strong data-start="892" data-end="915"
                                  >FAISS (HNSW or IVF)</strong
                                >
                                OR
                                <strong data-start="919" data-end="931"
                                  >Pinecone</strong
                                >
                              </td>
                              <td>
                                Elasticsearch
                                <strong data-start="948" data-end="962"
                                  >slows down</strong
                                >, so FAISS or Pinecone are better for handling
                                high-dimensional vectors.
                              </td>
                            </tr>
                            <tr data-start="1037" data-end="1258">
                              <td>
                                <strong data-start="1039" data-end="1048"
                                  >1024+</strong
                                >
                              </td>
                              <td>
                                <strong data-start="1051" data-end="1109"
                                  >AI-powered similarity search (GPT, complex
                                  embeddings)</strong
                                >
                              </td>
                              <td>
                                ✅
                                <strong data-start="1114" data-end="1144"
                                  >Milvus, Weaviate, or Vespa</strong
                                >
                              </td>
                              <td>
                                Elasticsearch
                                <strong data-start="1161" data-end="1180"
                                  >is not designed</strong
                                >
                                for this. Milvus or Weaviate
                                <strong data-start="1210" data-end="1255"
                                  >handle ultra-high-dim vectors
                                  efficiently</strong
                                >.
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <hr data-start="1260" data-end="1263" />
                        <h3 data-start="1265" data-end="1307">
                          <strong data-start="1269" data-end="1307"
                            >Detailed Breakdown of Alternatives</strong
                          >
                        </h3>
                        <p data-start="1308" data-end="1338">
                          🚀
                          <strong data-start="1311" data-end="1338"
                            >If you're dealing with:</strong
                          >
                        </p>
                        <h4 data-start="1340" data-end="1398">
                          <strong data-start="1345" data-end="1398"
                            >✅ 10–50 Dimensions → Elasticsearch (BM25,
                            TF-IDF)</strong
                          >
                        </h4>
                        <ul data-start="1399" data-end="1629">
                          <li data-start="1399" data-end="1483">
                            <strong data-start="1401" data-end="1413"
                              >Use Case</strong
                            >: Simple keyword-based searches, rule-based
                            matching, basic NLP tasks.
                          </li>
                          <li data-start="1484" data-end="1629">
                            <strong data-start="1486" data-end="1501"
                              >Best Option</strong
                            >:
                            <strong data-start="1503" data-end="1520"
                              >Elasticsearch</strong
                            >
                            is optimal here. Traditional search methods like
                            <strong data-start="1570" data-end="1609"
                              >BM25, TF-IDF, or Jaccard similarity</strong
                            >
                            are very efficient.
                          </li>
                        </ul>
                        <h4 data-start="1631" data-end="1685">
                          <strong data-start="1636" data-end="1685"
                            >✅ 100–300 Dimensions → Elasticsearch OR
                            FAISS</strong
                          >
                        </h4>
                        <ul data-start="1686" data-end="1994">
                          <li data-start="1686" data-end="1785">
                            <strong data-start="1688" data-end="1700"
                              >Use Case</strong
                            >: Medium-complexity embeddings (SBERT, fast
                            semantic search, recommendation systems).
                          </li>
                          <li data-start="1786" data-end="1994">
                            <strong data-start="1788" data-end="1803"
                              >Best Option</strong
                            >:
                            <ul data-start="1809" data-end="1994">
                              <li data-start="1809" data-end="1897">
                                <strong data-start="1811" data-end="1843"
                                  >Elasticsearch (dense_vector)</strong
                                >
                                → If you need
                                <strong data-start="1858" data-end="1875"
                                  >hybrid search</strong
                                >
                                (keyword + vector).
                              </li>
                              <li data-start="1900" data-end="1994">
                                <strong data-start="1902" data-end="1918"
                                  >FAISS (HNSW)</strong
                                >
                                → If you need
                                <strong data-start="1933" data-end="1956"
                                  >purely vector-based</strong
                                >
                                search (faster for large datasets).
                              </li>
                            </ul>
                          </li>
                        </ul>
                        <h4 data-start="1996" data-end="2045">
                          <strong data-start="2001" data-end="2045"
                            >✅ 512–768 Dimensions → FAISS OR Pinecone</strong
                          >
                        </h4>
                        <ul data-start="2046" data-end="2425">
                          <li data-start="2046" data-end="2132">
                            <strong data-start="2048" data-end="2060"
                              >Use Case</strong
                            >: High-dimensional
                            <strong data-start="2079" data-end="2116"
                              >image, video, and text embeddings</strong
                            >
                            from AI models.
                          </li>
                          <li data-start="2133" data-end="2425">
                            <strong data-start="2135" data-end="2150"
                              >Best Option</strong
                            >:
                            <ul data-start="2156" data-end="2425">
                              <li data-start="2156" data-end="2239">
                                <strong data-start="2158" data-end="2181"
                                  >FAISS (HNSW or IVF)</strong
                                >
                                → If you have a
                                <strong data-start="2198" data-end="2225"
                                  >large number of vectors</strong
                                >
                                (millions).
                              </li>
                              <li data-start="2242" data-end="2334">
                                <strong data-start="2244" data-end="2256"
                                  >Pinecone</strong
                                >
                                → If you need a
                                <strong data-start="2273" data-end="2292"
                                  >managed service</strong
                                >
                                for production-ready similarity search.
                              </li>
                              <li data-start="2337" data-end="2425">
                                <strong data-start="2339" data-end="2351"
                                  >Weaviate</strong
                                >
                                → If you need
                                <strong data-start="2366" data-end="2399"
                                  >hybrid search (text + vector)</strong
                                >
                                with built-in AI tools.
                              </li>
                            </ul>
                          </li>
                        </ul>
                        <h4 data-start="2427" data-end="2483">
                          <strong data-start="2432" data-end="2483"
                            >✅ 1024+ Dimensions → Milvus, Weaviate, or
                            Vespa</strong
                          >
                        </h4>
                        <ul data-start="2484" data-end="2891">
                          <li data-start="2484" data-end="2585">
                            <strong data-start="2486" data-end="2498"
                              >Use Case</strong
                            >: AI-powered
                            <strong data-start="2511" data-end="2584"
                              >semantic search, large-scale similarity search,
                              multimodal embeddings</strong
                            >.
                          </li>
                          <li data-start="2586" data-end="2891">
                            <strong data-start="2588" data-end="2603"
                              >Best Option</strong
                            >:
                            <ul data-start="2609" data-end="2891">
                              <li data-start="2609" data-end="2696">
                                <strong data-start="2611" data-end="2621"
                                  >Milvus</strong
                                >
                                → Best for
                                <strong data-start="2633" data-end="2656"
                                  >billions of vectors</strong
                                >, GPU acceleration, scalable indexing.
                              </li>
                              <li data-start="2699" data-end="2795">
                                <strong data-start="2701" data-end="2713"
                                  >Weaviate</strong
                                >
                                → Best for
                                <strong data-start="2725" data-end="2742"
                                  >hybrid search</strong
                                >, integrates with
                                <strong data-start="2760" data-end="2792"
                                  >OpenAI, Cohere, Hugging Face</strong
                                >.
                              </li>
                              <li data-start="2798" data-end="2891">
                                <strong data-start="2800" data-end="2809"
                                  >Vespa</strong
                                >
                                → Best for
                                <strong data-start="2821" data-end="2859"
                                  >real-time search &amp;
                                  recommendations</strong
                                >, used by
                                <strong data-start="2869" data-end="2888"
                                  >Spotify &amp; Yahoo</strong
                                >.
                              </li>
                            </ul>
                          </li>
                        </ul>
                        <hr data-start="2893" data-end="2896" />
                        <h3 data-start="2898" data-end="2940">
                          <strong data-start="2902" data-end="2940"
                            >Final Verdict: When to Choose Each</strong
                          >
                        </h3>
                        <table data-start="2941" data-end="3412">
                          <thead data-start="2941" data-end="2981">
                            <tr data-start="2941" data-end="2981">
                              <th data-start="2941" data-end="2962">
                                <strong data-start="2943" data-end="2961"
                                  >If You Need...</strong
                                >
                              </th>
                              <th data-start="2962" data-end="2981">
                                <strong data-start="2964" data-end="2979"
                                  >Best Choice</strong
                                >
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="3020" data-end="3412">
                            <tr data-start="3020" data-end="3091">
                              <td>
                                <strong data-start="3022" data-end="3052"
                                  >Traditional keyword search</strong
                                >
                              </td>
                              <td>
                                ✅
                                <strong data-start="3057" data-end="3089"
                                  >Elasticsearch (BM25, TF-IDF)</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="3092" data-end="3180">
                              <td>
                                <strong data-start="3094" data-end="3135"
                                  >Fast vector search for small datasets</strong
                                >
                              </td>
                              <td>
                                ✅
                                <strong data-start="3140" data-end="3178"
                                  >Elasticsearch (dense_vector, k-NN)</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="3181" data-end="3252">
                              <td>
                                <strong data-start="3183" data-end="3224"
                                  >Scalable AI-powered similarity search</strong
                                >
                              </td>
                              <td>
                                ✅
                                <strong data-start="3229" data-end="3250"
                                  >FAISS (HNSW, IVF)</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="3253" data-end="3325">
                              <td>
                                <strong data-start="3255" data-end="3289"
                                  >Production-ready vector search</strong
                                >
                              </td>
                              <td>
                                ✅
                                <strong data-start="3294" data-end="3323"
                                  >Pinecone, Weaviate, Vespa</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="3326" data-end="3412">
                              <td>
                                <strong data-start="3328" data-end="3378"
                                  >Ultra-high-dimensional vector search (1024+
                                  D)</strong
                                >
                              </td>
                              <td>
                                ✅
                                <strong data-start="3383" data-end="3410"
                                  >Milvus, Weaviate, Vespa</strong
                                >
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <p
                          data-start="3414"
                          data-end="3487"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like help
                          <strong data-start="3434" data-end="3483"
                            >choosing a specific solution for your
                            dataset</strong
                          >? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-18"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="581c5221-107f-4687-b13e-64945533b5c6"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          Why to reduce vector dimensions?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-19"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="9de57574-e08e-486d-8822-9af0ae3b5dc1"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="37">
                          <strong data-start="4" data-end="37"
                            >Why Reduce Vector Dimensions?</strong
                          >
                        </h3>
                        <p data-start="38" data-end="417">
                          Reducing the dimensionality of vectors is important
                          for
                          <strong data-start="94" data-end="122"
                            >performance optimization</strong
                          >
                          and
                          <strong data-start="127" data-end="149"
                            >storage efficiency</strong
                          >
                          while maintaining as much useful information as
                          possible. High-dimensional vectors (e.g.,
                          <strong data-start="240" data-end="259"
                            >512+ dimensions</strong
                          >) can slow down search operations, increase storage
                          costs, and lead to
                          <strong data-start="330" data-end="361"
                            >the curse of dimensionality</strong
                          >—where distances between vectors become less
                          meaningful.
                        </p>
                        <hr data-start="419" data-end="422" />
                        <h2 data-start="424" data-end="466">
                          <strong data-start="427" data-end="466"
                            >Reasons to Reduce Vector Dimensions</strong
                          >
                        </h2>
                        <h3 data-start="467" data-end="507">
                          <strong data-start="471" data-end="507"
                            >1. Speed Up Search and Retrieval</strong
                          >
                        </h3>
                        <ul data-start="508" data-end="689">
                          <li data-start="508" data-end="596">
                            <strong data-start="510" data-end="538"
                              >High-dimensional vectors</strong
                            >
                            increase the time required to find the nearest
                            neighbors.
                          </li>
                          <li data-start="597" data-end="689">
                            <strong data-start="599" data-end="628"
                              >Lower-dimensional vectors</strong
                            >
                            result in
                            <strong data-start="639" data-end="661"
                              >faster query times</strong
                            >
                            and
                            <strong data-start="666" data-end="688"
                              >efficient indexing</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="691" data-end="831">
                          📌
                          <strong data-start="694" data-end="706"
                            >Example:</strong
                          >
                          Searching in
                          <strong data-start="720" data-end="729">FAISS</strong>
                          with
                          <strong data-start="735" data-end="751"
                            >128D vectors</strong
                          >
                          is much faster than
                          <strong data-start="772" data-end="789"
                            >1024D vectors</strong
                          >, especially with
                          <strong data-start="807" data-end="830"
                            >billions of entries</strong
                          >.
                        </p>
                        <hr data-start="833" data-end="836" />
                        <h3 data-start="838" data-end="880">
                          <strong data-start="842" data-end="880"
                            >2. Reduce Memory and Storage Usage</strong
                          >
                        </h3>
                        <ul data-start="881" data-end="1004">
                          <li data-start="881" data-end="945">
                            Each vector
                            <strong data-start="895" data-end="921"
                              >requires storage space</strong
                            >
                            in a database or index.
                          </li>
                          <li data-start="946" data-end="1004">
                            A
                            <strong data-start="950" data-end="1003"
                              >512D vector uses 4x more space than a 128D
                              vector</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="1006" data-end="1023">
                          📌
                          <strong data-start="1009" data-end="1021"
                            >Example:</strong
                          >
                        </p>
                        <ul data-start="1024" data-end="1138">
                          <li data-start="1024" data-end="1080">
                            <strong data-start="1026" data-end="1041"
                              >128D vector</strong
                            >
                            (float32) →
                            <strong data-start="1054" data-end="1078"
                              >512 bytes per vector</strong
                            >
                          </li>
                          <li data-start="1081" data-end="1138">
                            <strong data-start="1083" data-end="1098"
                              >512D vector</strong
                            >
                            (float32) →
                            <strong data-start="1111" data-end="1136"
                              >2048 bytes per vector</strong
                            >
                          </li>
                        </ul>
                        <p data-start="1140" data-end="1200">
                          For
                          <strong data-start="1144" data-end="1167"
                            >millions of vectors</strong
                          >, this adds up
                          <strong data-start="1182" data-end="1199"
                            >significantly</strong
                          >.
                        </p>
                        <hr data-start="1202" data-end="1205" />
                        <h3 data-start="1207" data-end="1251">
                          <strong data-start="1211" data-end="1251"
                            >3. Avoid the Curse of Dimensionality</strong
                          >
                        </h3>
                        <ul data-start="1252" data-end="1385">
                          <li data-start="1252" data-end="1326">
                            In very high-dimensional spaces,
                            <strong data-start="1287" data-end="1325"
                              >all vectors tend to be equidistant</strong
                            >.
                          </li>
                          <li data-start="1327" data-end="1385">
                            This
                            <strong data-start="1334" data-end="1363"
                              >reduces the effectiveness</strong
                            >
                            of similarity search.
                          </li>
                        </ul>
                        <p data-start="1387" data-end="1540">
                          📌
                          <strong data-start="1390" data-end="1402"
                            >Example:</strong
                          >
                          If you're using
                          <strong data-start="1419" data-end="1441"
                            >Euclidean distance</strong
                          >, differences between
                          <strong data-start="1463" data-end="1515"
                            >similar and dissimilar vectors become
                            negligible</strong
                          >
                          in
                          <strong data-start="1519" data-end="1539"
                            >1000+ dimensions</strong
                          >.
                        </p>
                        <hr data-start="1542" data-end="1545" />
                        <h3 data-start="1547" data-end="1593">
                          <strong data-start="1551" data-end="1593"
                            >4. Improve Generalization and Accuracy</strong
                          >
                        </h3>
                        <ul data-start="1594" data-end="1746">
                          <li data-start="1594" data-end="1661">
                            Some dimensions contain
                            <strong data-start="1620" data-end="1629"
                              >noise</strong
                            >
                            rather than useful information.
                          </li>
                          <li data-start="1662" data-end="1746">
                            Reducing dimensions
                            <strong data-start="1684" data-end="1712"
                              >removes unnecessary data</strong
                            >, improving
                            <strong data-start="1724" data-end="1745"
                              >model performance</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="1748" data-end="1890">
                          📌
                          <strong data-start="1751" data-end="1763"
                            >Example:</strong
                          >
                          <strong data-start="1764" data-end="1802"
                            >PCA (Principal Component Analysis)</strong
                          >
                          helps
                          <strong data-start="1809" data-end="1853"
                            >eliminate weakly contributing dimensions</strong
                          >
                          while keeping meaningful variations.
                        </p>
                        <hr data-start="1892" data-end="1895" />
                        <h2 data-start="1897" data-end="1936">
                          <strong data-start="1900" data-end="1936"
                            >How to Reduce Vector Dimensions?</strong
                          >
                        </h2>
                        <h3 data-start="1937" data-end="1982">
                          <strong data-start="1941" data-end="1982"
                            >1. Principal Component Analysis (PCA)</strong
                          >
                        </h3>
                        <ul data-start="1983" data-end="2119">
                          <li data-start="1983" data-end="2051">
                            Identifies the most important features and removes
                            redundant ones.
                          </li>
                          <li data-start="2052" data-end="2119">
                            Reduces dimensionality while preserving most of the
                            <strong data-start="2106" data-end="2118"
                              >variance</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="2121" data-end="2175">
                          📌
                          <strong data-start="2124" data-end="2175"
                            >Example: Reduce a 512D vector to 128D using
                            PCA</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="2176"
                          data-end="2472"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from sklearn.decomposition import PCA
import numpy as np

vectors = np.random.rand(1000, 512)  # 1000 vectors with 512 dimensions
pca = PCA(n_components=128)  # Reduce to 128 dimensions
reduced_vectors = pca.fit_transform(vectors)
print(reduced_vectors.shape)  # Output: (1000, 128)
</span></code></div></div></pre>
                        <hr data-start="2474" data-end="2477" />
                        <h3 data-start="2479" data-end="2535">
                          <strong data-start="2483" data-end="2535"
                            >2. Autoencoders (Neural Network-Based
                            Reduction)</strong
                          >
                        </h3>
                        <ul data-start="2536" data-end="2665">
                          <li data-start="2536" data-end="2612">
                            Uses a
                            <strong data-start="2545" data-end="2568"
                              >deep learning model</strong
                            >
                            to compress vectors into a lower dimension.
                          </li>
                          <li data-start="2613" data-end="2665">
                            Works better than PCA for
                            <strong data-start="2641" data-end="2655"
                              >non-linear</strong
                            >
                            datasets.
                          </li>
                        </ul>
                        <p data-start="2667" data-end="2732">
                          📌
                          <strong data-start="2670" data-end="2732"
                            >Example: Using an Autoencoder for Dimensionality
                            Reduction</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="2733"
                          data-end="2923"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from tensorflow import keras

encoder = keras.Sequential([
    keras.layers.Dense(256, activation='relu', input_shape=(512,)),
    keras.layers.Dense(128, activation='relu')
])
</span></code></div></div></pre>
                        <p data-start="2925" data-end="3019">
                          ✅
                          <strong data-start="2927" data-end="2968"
                            >Better than PCA for complex datasets.</strong
                          ><br data-start="2968" data-end="2971" />❌
                          <strong data-start="2973" data-end="3019"
                            >Requires training data &amp; more
                            computation.</strong
                          >
                        </p>
                        <hr data-start="3021" data-end="3024" />
                        <h3 data-start="3026" data-end="3067">
                          <strong data-start="3030" data-end="3067"
                            >3. Feature Selection / Truncation</strong
                          >
                        </h3>
                        <ul data-start="3068" data-end="3197">
                          <li data-start="3068" data-end="3124">
                            Simply
                            <strong data-start="3077" data-end="3123"
                              >removes less significant vector
                              components</strong
                            >.
                          </li>
                          <li data-start="3125" data-end="3197">
                            Works well if
                            <strong data-start="3141" data-end="3196"
                              >only a subset of features contributes to
                              similarity</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="3199" data-end="3277">
                          📌
                          <strong data-start="3202" data-end="3214"
                            >Example:</strong
                          >
                          If using
                          <strong data-start="3224" data-end="3250"
                            >BERT embeddings (768D)</strong
                          >, keep only
                          <strong data-start="3262" data-end="3276"
                            >first 300D</strong
                          >:
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="3278"
                          data-end="3359"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>vector_bert = vector_bert[:300]  # Truncate to first 300 dimensions
</span></code></div></div></pre>
                        <p data-start="3360" data-end="3423">
                          ✅
                          <strong data-start="3362" data-end="3379"
                            >Fast &amp; simple</strong
                          ><br data-start="3379" data-end="3382" />❌
                          <strong data-start="3384" data-end="3423"
                            >Might discard important information</strong
                          >
                        </p>
                        <hr data-start="3425" data-end="3428" />
                        <h2 data-start="3430" data-end="3481">
                          <strong data-start="3433" data-end="3481"
                            >Final Verdict: Should You Reduce
                            Dimensions?</strong
                          >
                        </h2>
                        <table data-start="3482" data-end="3863">
                          <thead data-start="3482" data-end="3519">
                            <tr data-start="3482" data-end="3519">
                              <th data-start="3482" data-end="3497">
                                <strong data-start="3484" data-end="3496"
                                  >Scenario</strong
                                >
                              </th>
                              <th data-start="3497" data-end="3519">
                                <strong data-start="3499" data-end="3517"
                                  >Recommendation</strong
                                >
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="3555" data-end="3863">
                            <tr data-start="3555" data-end="3647">
                              <td>
                                <strong data-start="3557" data-end="3598"
                                  >Using Elasticsearch for vector search</strong
                                >
                              </td>
                              <td>
                                ✅ Reduce to
                                <strong data-start="3613" data-end="3622"
                                  >≤300D</strong
                                >
                                (best for performance)
                              </td>
                            </tr>
                            <tr data-start="3648" data-end="3713">
                              <td>
                                <strong data-start="3650" data-end="3675"
                                  >Using FAISS or Milvus</strong
                                >
                              </td>
                              <td>
                                ✅ Keep
                                <strong data-start="3685" data-end="3697"
                                  >512D max</strong
                                >
                                unless needed
                              </td>
                            </tr>
                            <tr data-start="3714" data-end="3788">
                              <td>
                                <strong data-start="3716" data-end="3748"
                                  >Handling billions of vectors</strong
                                >
                              </td>
                              <td>
                                ✅ Reduce to
                                <strong data-start="3763" data-end="3776"
                                  >128D-256D</strong
                                >
                                for speed
                              </td>
                            </tr>
                            <tr data-start="3789" data-end="3863">
                              <td>
                                <strong data-start="3791" data-end="3816"
                                  >Need highest accuracy</strong
                                >
                              </td>
                              <td>
                                ❌ Do NOT reduce (keep original embeddings)
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <p
                          data-start="3865"
                          data-end="3943"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like help
                          <strong data-start="3885" data-end="3913"
                            >choosing the best method</strong
                          >
                          for your specific dataset? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-20"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="fd23dad1-a526-48a0-8d01-1fee4e249b1c"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          When using pca how to decide the number of the output
                          dimensions?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-21"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="183f250b-d5c7-4371-b040-ce4a291e25b3"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="61">
                          <strong data-start="4" data-end="61"
                            >How to Decide the Number of Output Dimensions in
                            PCA?</strong
                          >
                        </h3>
                        <p data-start="62" data-end="282">
                          When using
                          <strong data-start="73" data-end="111"
                            >Principal Component Analysis (PCA)</strong
                          >
                          for dimensionality reduction, the
                          <strong data-start="146" data-end="183"
                            >right number of output dimensions</strong
                          >
                          depends on how much
                          <strong data-start="204" data-end="230"
                            >information (variance)</strong
                          >
                          you want to keep while reducing computational cost.
                        </p>
                        <hr data-start="284" data-end="287" />
                        <h2 data-start="289" data-end="345">
                          <strong data-start="292" data-end="345"
                            >Key Approaches to Decide Output Dimensions in
                            PCA</strong
                          >
                        </h2>
                        <h3 data-start="346" data-end="404">
                          <strong data-start="350" data-end="404"
                            >1. Keep a Target Variance Percentage
                            (Recommended)</strong
                          >
                        </h3>
                        <ul data-start="405" data-end="861">
                          <li data-start="405" data-end="551">
                            <strong data-start="407" data-end="430"
                              >PCA transforms data</strong
                            >
                            into a new space where each
                            <strong data-start="459" data-end="482"
                              >principal component</strong
                            >
                            (dimension) explains a certain amount of
                            <strong data-start="524" data-end="536"
                              >variance</strong
                            >
                            (information).
                          </li>
                          <li data-start="552" data-end="656">
                            The
                            <strong data-start="558" data-end="566"
                              >goal</strong
                            >
                            is to
                            <strong data-start="573" data-end="617"
                              >retain most of the important information</strong
                            >
                            while removing less useful dimensions.
                          </li>
                          <li data-start="657" data-end="861">
                            A
                            <strong data-start="661" data-end="683"
                              >good rule of thumb</strong
                            >:
                            <ul data-start="687" data-end="861">
                              <li data-start="687" data-end="750">
                                <strong data-start="689" data-end="708"
                                  >90–95% variance</strong
                                >
                                → Keeps almost all important information.
                              </li>
                              <li data-start="753" data-end="809">
                                <strong data-start="755" data-end="774"
                                  >80–85% variance</strong
                                >
                                → Balances performance &amp; accuracy.
                              </li>
                              <li data-start="812" data-end="861">
                                <strong data-start="814" data-end="831"
                                  >&lt;80% variance</strong
                                >
                                → May lose important details.
                              </li>
                            </ul>
                          </li>
                        </ul>
                        <p data-start="863" data-end="934">
                          📌
                          <strong data-start="866" data-end="934"
                            >Example: Choosing the Number of Components Based on
                            95% Variance</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="935"
                          data-end="1458"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from sklearn.decomposition import PCA
import numpy as np

# Generate random high-dimensional data (1000 samples, 512 features)
data = np.random.rand(1000, 512)

# Fit PCA without specifying dimensions
pca = PCA().fit(data)

# Calculate cumulative explained variance
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

# Find the number of dimensions needed to keep 95% variance
n_components = np.argmax(cumulative_variance &gt;= 0.95) + 1
print(f"Optimal dimensions for 95% variance: {n_components}")
</span></code></div></div></pre>
                        <p data-start="1459" data-end="1599">
                          ✅
                          <strong data-start="1461" data-end="1475"
                            >Advantage:</strong
                          >
                          Ensures minimal
                          <strong data-start="1492" data-end="1515"
                            >loss of information</strong
                          >
                          while improving
                          <strong data-start="1532" data-end="1546"
                            >efficiency</strong
                          >.<br data-start="1547" data-end="1550" />❌
                          <strong data-start="1552" data-end="1569"
                            >Disadvantage:</strong
                          >
                          Requires
                          <strong data-start="1579" data-end="1598"
                            >trial and error</strong
                          >.
                        </p>
                        <hr data-start="1601" data-end="1604" />
                        <h3 data-start="1606" data-end="1654">
                          <strong data-start="1610" data-end="1654"
                            >2. Use an "Elbow Method" on a Scree Plot</strong
                          >
                        </h3>
                        <ul data-start="1655" data-end="1808">
                          <li data-start="1655" data-end="1721">
                            A
                            <strong data-start="1659" data-end="1673"
                              >scree plot</strong
                            >
                            shows the
                            <strong data-start="1684" data-end="1720"
                              >explained variance per component</strong
                            >.
                          </li>
                          <li data-start="1722" data-end="1808">
                            The
                            <strong data-start="1728" data-end="1745"
                              >"elbow point"</strong
                            >
                            is where adding more dimensions gives
                            <strong data-start="1784" data-end="1807"
                              >diminishing returns</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="1810" data-end="1873">
                          📌
                          <strong data-start="1813" data-end="1873"
                            >Example: Plotting PCA Components to Find the Elbow
                            Point</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1874"
                          data-end="2175"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import matplotlib.pyplot as plt

# Plot variance explained by each component
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA Scree Plot')
plt.grid()
plt.show()
</span></code></div></div></pre>
                        <p data-start="2176" data-end="2314">
                          ✅
                          <strong data-start="2178" data-end="2192"
                            >Advantage:</strong
                          >
                          <strong data-start="2193" data-end="2203"
                            >Visual</strong
                          >
                          and
                          <strong data-start="2208" data-end="2221"
                            >intuitive</strong
                          >
                          way to pick the right dimensions.<br
                            data-start="2255"
                            data-end="2258"
                          />❌
                          <strong data-start="2260" data-end="2277"
                            >Disadvantage:</strong
                          >
                          May not be
                          <strong data-start="2289" data-end="2300"
                            >precise</strong
                          >
                          in all cases.
                        </p>
                        <hr data-start="2316" data-end="2319" />
                        <h3 data-start="2321" data-end="2370">
                          <strong data-start="2325" data-end="2370"
                            >3. Set a Fixed Number Based on Experience</strong
                          >
                        </h3>
                        <p data-start="2371" data-end="2467">
                          If you don’t want to calculate variance or plot a
                          scree plot, you can use
                          <strong data-start="2445" data-end="2466"
                            >common benchmarks</strong
                          >:
                        </p>
                        <table data-start="2469" data-end="2778">
                          <thead data-start="2469" data-end="2529">
                            <tr data-start="2469" data-end="2529">
                              <th data-start="2469" data-end="2495">
                                <strong data-start="2471" data-end="2494"
                                  >Original Dimensions</strong
                                >
                              </th>
                              <th data-start="2495" data-end="2529">
                                <strong data-start="2497" data-end="2527"
                                  >Recommended PCA Dimensions</strong
                                >
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="2578" data-end="2778">
                            <tr data-start="2578" data-end="2609">
                              <td>
                                <strong data-start="2580" data-end="2590"
                                  >50–100</strong
                                >
                              </td>
                              <td>
                                <strong data-start="2593" data-end="2607"
                                  >Keep 20–50</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="2610" data-end="2645">
                              <td>
                                <strong data-start="2612" data-end="2619"
                                  >256</strong
                                >
                              </td>
                              <td>
                                <strong data-start="2622" data-end="2643"
                                  >Reduce to 128–200</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="2646" data-end="2681">
                              <td>
                                <strong data-start="2648" data-end="2655"
                                  >512</strong
                                >
                              </td>
                              <td>
                                <strong data-start="2658" data-end="2679"
                                  >Reduce to 128–300</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="2682" data-end="2735">
                              <td>
                                <strong data-start="2684" data-end="2709"
                                  >768 (BERT embeddings)</strong
                                >
                              </td>
                              <td>
                                <strong data-start="2712" data-end="2733"
                                  >Reduce to 300–500</strong
                                >
                              </td>
                            </tr>
                            <tr data-start="2736" data-end="2778">
                              <td>
                                <strong data-start="2738" data-end="2747"
                                  >1024+</strong
                                >
                              </td>
                              <td>
                                <strong data-start="2750" data-end="2776"
                                  >Reduce to 512 or lower</strong
                                >
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <p data-start="2780" data-end="2834">
                          📌
                          <strong data-start="2783" data-end="2834"
                            >Example: Directly Reducing 512D Vectors to
                            128D</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="2835"
                          data-end="2915"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>pca = PCA(n_components=128)
data_reduced = pca.fit_transform(data)
</span></code></div></div></pre>
                        <p data-start="2916" data-end="3031">
                          ✅
                          <strong data-start="2918" data-end="2932"
                            >Advantage:</strong
                          >
                          <strong data-start="2933" data-end="2943"
                            >Simple</strong
                          >
                          and works well for most cases.<br
                            data-start="2974"
                            data-end="2977"
                          />❌
                          <strong data-start="2979" data-end="2996"
                            >Disadvantage:</strong
                          >
                          <strong data-start="2997" data-end="3030"
                            >Not optimal for every dataset</strong
                          >.
                        </p>
                        <hr data-start="3033" data-end="3036" />
                        <h3 data-start="3038" data-end="3089">
                          <strong data-start="3042" data-end="3089"
                            >4. Keep Only the Top "K" Important Features</strong
                          >
                        </h3>
                        <ul data-start="3090" data-end="3263">
                          <li data-start="3090" data-end="3184">
                            If certain
                            <strong data-start="3103" data-end="3148"
                              >features (dimensions) contribute the most</strong
                            >
                            to similarity, keep only
                            <strong data-start="3174" data-end="3183"
                              >those</strong
                            >.
                          </li>
                          <li data-start="3185" data-end="3263">
                            Use
                            <strong data-start="3191" data-end="3227"
                              >PCA's feature importance ranking</strong
                            >
                            to
                            <strong data-start="3231" data-end="3262"
                              >drop unimportant dimensions</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="3265" data-end="3320">
                          📌
                          <strong data-start="3268" data-end="3320"
                            >Example: Selecting the Top 50 Important
                            Features</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="3321"
                          data-end="3447"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>important_features = np.argsort(-pca.explained_variance_ratio_)[:50]
data_selected = data[:, important_features]
</span></code></div></div></pre>
                        <p data-start="3448" data-end="3574">
                          ✅
                          <strong data-start="3450" data-end="3464"
                            >Advantage:</strong
                          >
                          Keeps
                          <strong data-start="3471" data-end="3497"
                            >only the most relevant</strong
                          >
                          information.<br data-start="3510" data-end="3513" />❌
                          <strong data-start="3515" data-end="3532"
                            >Disadvantage:</strong
                          >
                          May require
                          <strong data-start="3545" data-end="3573"
                            >manual feature selection</strong
                          >.
                        </p>
                        <hr data-start="3576" data-end="3579" />
                        <h2 data-start="3581" data-end="3609">
                          <strong data-start="3584" data-end="3609"
                            >Final Recommendations</strong
                          >
                        </h2>
                        <table data-start="3610" data-end="4096">
                          <thead data-start="3610" data-end="3656">
                            <tr data-start="3610" data-end="3656">
                              <th data-start="3610" data-end="3625">
                                <strong data-start="3612" data-end="3624"
                                  >Use Case</strong
                                >
                              </th>
                              <th data-start="3625" data-end="3656">
                                <strong data-start="3627" data-end="3654"
                                  >How to Pick Dimensions?</strong
                                >
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="3692" data-end="4096">
                            <tr data-start="3692" data-end="3787">
                              <td>
                                <strong data-start="3694" data-end="3724"
                                  >Want to maximize accuracy?</strong
                                >
                              </td>
                              <td>
                                ✅ Keep
                                <strong data-start="3734" data-end="3750"
                                  >95% variance</strong
                                >
                                (best balance of speed &amp; quality).
                              </td>
                            </tr>
                            <tr data-start="3788" data-end="3896">
                              <td>
                                <strong data-start="3790" data-end="3823"
                                  >Want to optimize performance?</strong
                                >
                              </td>
                              <td>
                                ✅ Keep
                                <strong data-start="3833" data-end="3852"
                                  >85–90% variance</strong
                                >
                                (faster search, slight quality tradeoff).
                              </td>
                            </tr>
                            <tr data-start="3897" data-end="3996">
                              <td>
                                <strong data-start="3899" data-end="3930"
                                  >Need a quick approximation?</strong
                                >
                              </td>
                              <td>
                                ✅ Use
                                <strong data-start="3939" data-end="3962"
                                  >preset fixed values</strong
                                >
                                (e.g., reduce
                                <strong data-start="3977" data-end="3992"
                                  >512D → 128D</strong
                                >).
                              </td>
                            </tr>
                            <tr data-start="3997" data-end="4096">
                              <td>
                                <strong data-start="3999" data-end="4051"
                                  >Have a large dataset &amp; need the best
                                  efficiency?</strong
                                >
                              </td>
                              <td>
                                ✅ Use the
                                <strong data-start="4064" data-end="4093"
                                  >Scree Plot / Elbow Method</strong
                                >.
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <p
                          data-start="4098"
                          data-end="4183"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like me to
                          <strong data-start="4119" data-end="4146"
                            >run PCA on your dataset</strong
                          >
                          or generate a scree plot for you? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-22"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="c80b7dac-1971-4e4e-afce-d4c9091cd677"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          How is reducing 512 to 128 is 95 percent variance?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-23"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="87e7b7d8-e00a-424a-aebe-b868665105f0"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <p data-start="0" data-end="239">
                          Reducing
                          <strong data-start="9" data-end="34"
                            >512 dimensions to 128</strong
                          >
                          <strong data-start="35" data-end="59"
                            >does not necessarily</strong
                          >
                          mean
                          <strong data-start="65" data-end="81"
                            >95% variance</strong
                          >
                          is preserved. The number of dimensions you should keep
                          depends on
                          <strong data-start="148" data-end="206"
                            >how much variance each principal component
                            contributes</strong
                          >, which is
                          <strong data-start="217" data-end="237"
                            >dataset-specific</strong
                          >.
                        </p>
                        <h3 data-start="241" data-end="291">
                          <strong data-start="245" data-end="291"
                            >Why 512 → 128 Might Not Mean 95% Variance?</strong
                          >
                        </h3>
                        <p data-start="292" data-end="465">
                          PCA selects the
                          <strong data-start="308" data-end="326"
                            >most important</strong
                          >
                          components that
                          <strong data-start="343" data-end="375"
                            >capture the highest variance</strong
                          >. However, variance is
                          <strong data-start="398" data-end="424"
                            >not evenly distributed</strong
                          >
                          across all dimensions. In some datasets:
                        </p>
                        <ul data-start="466" data-end="618">
                          <li data-start="466" data-end="532">
                            The
                            <strong data-start="472" data-end="496"
                              >first few components</strong
                            >
                            might explain most of the variance.
                          </li>
                          <li data-start="533" data-end="618">
                            In other cases,
                            <strong data-start="551" data-end="570"
                              >many dimensions</strong
                            >
                            might contribute
                            <strong data-start="588" data-end="617"
                              >small amounts of variance</strong
                            >.
                          </li>
                        </ul>
                        <hr data-start="620" data-end="623" />
                        <h3 data-start="625" data-end="680">
                          <strong data-start="629" data-end="680"
                            >How to Check the Variance Before Choosing
                            128D?</strong
                          >
                        </h3>
                        <p data-start="681" data-end="761">
                          Instead of
                          <strong data-start="692" data-end="704"
                            >assuming</strong
                          >
                          128D will keep 95% variance, we should
                          <strong data-start="744" data-end="760"
                            >calculate it</strong
                          >.
                        </p>
                        <p data-start="763" data-end="834">
                          📌
                          <strong data-start="766" data-end="834"
                            >Example: Finding the Right Number of Dimensions for
                            95% Variance</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="835"
                          data-end="1359"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from sklearn.decomposition import PCA
import numpy as np

# Generate random high-dimensional data (1000 samples, 512 features)
data = np.random.rand(1000, 512)

# Fit PCA without specifying components
pca = PCA().fit(data)

# Calculate cumulative variance
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

# Find the number of components needed for 95% variance
n_components_95 = np.argmax(cumulative_variance &gt;= 0.95) + 1
print(f"Number of dimensions needed for 95% variance: {n_components_95}")
</span></code></div></div></pre>
                        <p data-start="1360" data-end="1453">
                          This will give
                          <strong data-start="1375" data-end="1409"
                            >the exact number of dimensions</strong
                          >
                          needed to preserve
                          <strong data-start="1429" data-end="1452"
                            >95% of the variance</strong
                          >.
                        </p>
                        <hr data-start="1455" data-end="1458" />
                        <h3 data-start="1460" data-end="1494">
                          <strong data-start="1464" data-end="1494"
                            >What If 128D Is Too Small?</strong
                          >
                        </h3>
                        <p data-start="1495" data-end="1608">
                          If
                          <code data-start="1498" data-end="1521"
                            >n_components_95 &gt; 128</code
                          >, then reducing to
                          <strong data-start="1540" data-end="1593"
                            >128D might result in losing important
                            information</strong
                          >. In that case:
                        </p>
                        <ul data-start="1609" data-end="1753">
                          <li data-start="1609" data-end="1689">
                            <strong data-start="1611" data-end="1641"
                              >If
                              <code data-start="1616" data-end="1639"
                                >n_components_95 = 250</code
                              ></strong
                            >, you should
                            <strong data-start="1654" data-end="1688"
                              >reduce to 250D instead of 128D</strong
                            >.
                          </li>
                          <li data-start="1690" data-end="1753">
                            <strong data-start="1692" data-end="1722"
                              >If
                              <code data-start="1697" data-end="1720"
                                >n_components_95 = 120</code
                              ></strong
                            >, reducing to
                            <strong data-start="1736" data-end="1752"
                              >128D is fine</strong
                            >.
                          </li>
                        </ul>
                        <hr data-start="1755" data-end="1758" />
                        <h3 data-start="1760" data-end="1841">
                          <strong data-start="1764" data-end="1841"
                            >Better Approach: Always Calculate the Right
                            Dimension Instead of Assuming</strong
                          >
                        </h3>
                        <p data-start="1842" data-end="1983">
                          ✅ Run
                          <strong data-start="1848" data-end="1885"
                            >PCA without specifying dimensions</strong
                          >.<br data-start="1886" data-end="1889" />✅ Find
                          <strong data-start="1896" data-end="1937"
                            >where cumulative variance reaches 95%</strong
                          >.<br data-start="1938" data-end="1941" />✅ Use that
                          as the
                          <strong data-start="1959" data-end="1980"
                            >optimal dimension</strong
                          >.
                        </p>
                        <p
                          data-start="1985"
                          data-end="2085"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like me to
                          <strong data-start="2006" data-end="2034"
                            >run this on your dataset</strong
                          >
                          to find the optimal dimension for 95% variance? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-24"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="4af4f2cc-bbc5-4e4f-a839-aa27cd5dce3f"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          Using the PCA without specifying dimensions is ok?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-25"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="74b92741-b0b0-4bb9-9ea2-3f2fa45b2381"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <p data-start="0" data-end="176">
                          Yes!
                          <strong data-start="5" data-end="67"
                            >Using PCA without specifying dimensions is
                            completely fine</strong
                          >
                          and is actually the
                          <strong data-start="88" data-end="100"
                            >best way</strong
                          >
                          to determine the
                          <strong data-start="118" data-end="150"
                            >optimal number of dimensions</strong
                          >
                          for preserving variance.
                        </p>
                        <hr data-start="178" data-end="181" />
                        <h2 data-start="183" data-end="246">
                          <strong data-start="186" data-end="246"
                            >Why Is It Okay to Use PCA Without Specifying
                            Dimensions?</strong
                          >
                        </h2>
                        <ol data-start="247" data-end="927">
                          <li data-start="247" data-end="486">
                            <p data-start="250" data-end="294">
                              <strong data-start="250" data-end="292"
                                >PCA Computes All Components by Default</strong
                              >
                            </p>
                            <ul data-start="298" data-end="486">
                              <li data-start="298" data-end="392">
                                If you don’t specify
                                <code data-start="321" data-end="335"
                                  >n_components</code
                                >,
                                <strong data-start="337" data-end="391"
                                  >PCA will compute all possible principal
                                  components</strong
                                >.
                              </li>
                              <li data-start="396" data-end="486">
                                You can then
                                <strong data-start="411" data-end="446"
                                  >analyze the cumulative variance</strong
                                >
                                and decide how many dimensions to keep.
                              </li>
                            </ul>
                          </li>
                          <li data-start="488" data-end="671">
                            <p data-start="491" data-end="542">
                              <strong data-start="491" data-end="540"
                                >You Can Choose the Best Dimension
                                Dynamically</strong
                              >
                            </p>
                            <ul data-start="546" data-end="671">
                              <li data-start="546" data-end="671">
                                Instead of guessing (e.g., assuming 128D is
                                enough), you can
                                <strong data-start="609" data-end="649"
                                  >calculate the exact dimension needed</strong
                                >
                                to keep 95% variance.
                              </li>
                            </ul>
                          </li>
                          <li data-start="673" data-end="927">
                            <p data-start="676" data-end="717">
                              <strong data-start="676" data-end="715"
                                >Avoids Unnecessary Information Loss</strong
                              >
                            </p>
                            <ul data-start="721" data-end="927">
                              <li data-start="721" data-end="812">
                                If you set
                                <code data-start="734" data-end="752"
                                  >n_components=128</code
                                >
                                without checking, you might
                                <strong data-start="781" data-end="811"
                                  >lose important information</strong
                                >.
                              </li>
                              <li data-start="816" data-end="927">
                                By running
                                <strong data-start="829" data-end="847"
                                  >full PCA first</strong
                                >, you can
                                <strong data-start="857" data-end="895"
                                  >see the full variance distribution</strong
                                >
                                and make a
                                <strong data-start="907" data-end="926"
                                  >better decision</strong
                                >.
                              </li>
                            </ul>
                          </li>
                        </ol>
                        <hr data-start="929" data-end="932" />
                        <h2 data-start="934" data-end="985">
                          <strong data-start="937" data-end="985"
                            >How to Run PCA Without Specifying
                            Dimensions</strong
                          >
                        </h2>
                        <p data-start="986" data-end="1029">
                          📌
                          <strong data-start="989" data-end="1029"
                            >Step 1: Fit PCA and Analyze Variance</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1030"
                          data-end="1560"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>from sklearn.decomposition import PCA
import numpy as np

# Simulate high-dimensional data (1000 samples, 512 features)
data = np.random.rand(1000, 512)

# Fit PCA without specifying n_components
pca = PCA().fit(data)

# Compute cumulative explained variance
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

# Find number of components needed to retain 95% variance
n_components_95 = np.argmax(cumulative_variance &gt;= 0.95) + 1
print(f"Optimal number of dimensions for 95% variance: {n_components_95}")
</span></code></div></div></pre>
                        <p data-start="1562" data-end="1613">
                          📌
                          <strong data-start="1565" data-end="1613"
                            >Step 2: Reduce Data to the Optimal
                            Dimension</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1614"
                          data-end="1824"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span># Apply PCA with the optimal number of dimensions
pca_optimal = PCA(n_components=n_components_95)
data_reduced = pca_optimal.fit_transform(data)

print(f"Reduced data shape: {data_reduced.shape}")
</span></code></div></div></pre>
                        <hr data-start="1826" data-end="1829" />
                        <h2 data-start="1831" data-end="1884">
                          <strong data-start="1834" data-end="1884"
                            >What Happens If I Don't Reduce the
                            Dimensions?</strong
                          >
                        </h2>
                        <ul data-start="1885" data-end="2108">
                          <li data-start="1885" data-end="2016">
                            If you
                            <strong data-start="1894" data-end="1917"
                              >keep all components</strong
                            >
                            (512D in this case), your dataset remains
                            <strong data-start="1960" data-end="1980"
                              >high-dimensional</strong
                            >
                            and
                            <strong data-start="1985" data-end="2015"
                              >search performance suffers</strong
                            >.
                          </li>
                          <li data-start="2017" data-end="2108">
                            If you
                            <strong data-start="2026" data-end="2045"
                              >reduce too much</strong
                            >
                            (e.g., forcing 128D), you might
                            <strong data-start="2078" data-end="2107"
                              >lose critical information</strong
                            >.
                          </li>
                        </ul>
                        <hr data-start="2110" data-end="2113" />
                        <h2 data-start="2115" data-end="2136">
                          <strong data-start="2118" data-end="2136"
                            >Final Verdict:</strong
                          >
                        </h2>
                        <p data-start="2137" data-end="2401">
                          ✔
                          <strong data-start="2139" data-end="2206"
                            >Using PCA without specifying
                            <code data-start="2170" data-end="2184"
                              >n_components</code
                            >
                            is a good approach.</strong
                          ><br data-start="2206" data-end="2209" />✔
                          <strong data-start="2211" data-end="2261"
                            >First, compute variance across all
                            dimensions.</strong
                          ><br data-start="2261" data-end="2264" />✔
                          <strong data-start="2266" data-end="2332"
                            >Then, decide how many dimensions to keep (e.g., 95%
                            variance).</strong
                          ><br data-start="2332" data-end="2335" />✔
                          <strong data-start="2337" data-end="2399"
                            >Finally, apply PCA with the selected number of
                            dimensions.</strong
                          >
                        </p>
                        <p
                          data-start="2403"
                          data-end="2483"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like me to help with
                          <strong data-start="2434" data-end="2477"
                            >visualizing variance using a Scree plot</strong
                          >? 📊🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-26"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="59c1c29d-13d7-4c88-99bb-cf88b46e9083"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          cumulative_variance what is this?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-27"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="c2aef7ba-2f90-48f7-ae6f-035fb1701563"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="45">
                          <strong data-start="4" data-end="45"
                            >What Is
                            <code data-start="14" data-end="35"
                              >cumulative_variance</code
                            >
                            in PCA?</strong
                          >
                        </h3>
                        <p data-start="46" data-end="302">
                          <code data-start="46" data-end="67"
                            >cumulative_variance</code
                          >
                          refers to the
                          <strong data-start="82" data-end="129"
                            >cumulative sum of explained variance ratios</strong
                          >
                          from
                          <strong data-start="135" data-end="173"
                            >Principal Component Analysis (PCA)</strong
                          >. It helps determine how many principal components
                          (dimensions)
                          <strong data-start="237" data-end="278"
                            >retain the most important information</strong
                          >
                          from the original data.
                        </p>
                        <hr data-start="304" data-end="307" />
                        <h2 data-start="309" data-end="345">
                          <strong data-start="312" data-end="345"
                            >Breaking It Down Step by Step</strong
                          >
                        </h2>
                        <h3 data-start="346" data-end="381">
                          <strong data-start="350" data-end="381"
                            >1. Explained Variance Ratio</strong
                          >
                        </h3>
                        <p data-start="382" data-end="536">
                          When you apply PCA, each new component captures
                          <strong data-start="430" data-end="471"
                            >some percentage of the total variance</strong
                          >
                          in the dataset. This is called the
                          <strong data-start="507" data-end="535"
                            >explained variance ratio</strong
                          >.
                        </p>
                        <p data-start="538" data-end="578">
                          📌
                          <strong data-start="541" data-end="578"
                            >Example: Explained Variance Ratio</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="579"
                          data-end="651"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>pca = PCA().fit(data)
print(pca.explained_variance_ratio_)
</span></code></div></div></pre>
                        <p data-start="652" data-end="669">Output (example):</p>
                        <pre
                          class="!overflow-visible"
                          data-start="670"
                          data-end="713"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none"></div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre"><span>[0.40, 0.25, 0.15, 0.10, 0.05, ...]
</span></code></div></div></pre>
                        <p data-start="714" data-end="725">This means:</p>
                        <ul data-start="726" data-end="942">
                          <li data-start="726" data-end="787">
                            <strong data-start="728" data-end="753"
                              >PC1 (first component)</strong
                            >
                            explains
                            <strong data-start="763" data-end="786"
                              >40% of the variance</strong
                            >.
                          </li>
                          <li data-start="788" data-end="834">
                            <strong data-start="790" data-end="816"
                              >PC2 (second component)</strong
                            >
                            explains
                            <strong data-start="826" data-end="833">25%</strong
                            >.
                          </li>
                          <li data-start="835" data-end="880">
                            <strong data-start="837" data-end="862"
                              >PC3 (third component)</strong
                            >
                            explains
                            <strong data-start="872" data-end="879">15%</strong
                            >.
                          </li>
                          <li data-start="881" data-end="927">
                            <strong data-start="883" data-end="909"
                              >PC4 (fourth component)</strong
                            >
                            explains
                            <strong data-start="919" data-end="926">10%</strong
                            >.
                          </li>
                          <li data-start="928" data-end="942">And so on...</li>
                        </ul>
                        <hr data-start="944" data-end="947" />
                        <h3 data-start="949" data-end="1013">
                          <strong data-start="953" data-end="1013"
                            >2. Cumulative Explained Variance (<code
                              data-start="989"
                              data-end="1010"
                              >cumulative_variance</code
                            >)</strong
                          >
                        </h3>
                        <p data-start="1014" data-end="1171">
                          Instead of looking at
                          <strong data-start="1036" data-end="1067"
                            >each component individually</strong
                          >, we
                          <strong data-start="1072" data-end="1105"
                            >sum up the explained variance</strong
                          >
                          across components to see
                          <strong data-start="1131" data-end="1170"
                            >how much total variance is retained</strong
                          >.
                        </p>
                        <p data-start="1173" data-end="1209">
                          📌
                          <strong data-start="1176" data-end="1209"
                            >Computing Cumulative Variance</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1210"
                          data-end="1363"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import numpy as np

# Compute cumulative variance
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)
print(cumulative_variance)
</span></code></div></div></pre>
                        <p data-start="1364" data-end="1371">Output:</p>
                        <pre
                          class="!overflow-visible"
                          data-start="1372"
                          data-end="1415"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none"></div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre"><span>[0.40, 0.65, 0.80, 0.90, 0.95, ...]
</span></code></div></div></pre>
                        <p data-start="1416" data-end="1427">This means:</p>
                        <ul data-start="1428" data-end="1714">
                          <li data-start="1428" data-end="1481">
                            <strong data-start="1430" data-end="1451"
                              >First 1 component</strong
                            >
                            captures
                            <strong data-start="1461" data-end="1480"
                              >40% of variance</strong
                            >.
                          </li>
                          <li data-start="1482" data-end="1523">
                            <strong data-start="1484" data-end="1506"
                              >First 2 components</strong
                            >
                            capture
                            <strong data-start="1515" data-end="1522"
                              >65%</strong
                            >.
                          </li>
                          <li data-start="1524" data-end="1565">
                            <strong data-start="1526" data-end="1548"
                              >First 3 components</strong
                            >
                            capture
                            <strong data-start="1557" data-end="1564"
                              >80%</strong
                            >.
                          </li>
                          <li data-start="1566" data-end="1607">
                            <strong data-start="1568" data-end="1590"
                              >First 4 components</strong
                            >
                            capture
                            <strong data-start="1599" data-end="1606"
                              >90%</strong
                            >.
                          </li>
                          <li data-start="1608" data-end="1714">
                            <strong data-start="1610" data-end="1632"
                              >First 5 components</strong
                            >
                            capture
                            <strong data-start="1641" data-end="1648"
                              >95%</strong
                            >
                            (<strong data-start="1650" data-end="1712"
                              >✅ This is the number of dimensions needed for
                              95% variance</strong
                            >).
                          </li>
                        </ul>
                        <hr data-start="1716" data-end="1719" />
                        <h3 data-start="1721" data-end="1772">
                          <strong data-start="1725" data-end="1772"
                            >3. Finding the Optimal Number of Dimensions</strong
                          >
                        </h3>
                        <p data-start="1773" data-end="1855">
                          To find the
                          <strong data-start="1785" data-end="1818"
                            >smallest number of dimensions</strong
                          >
                          that retain
                          <strong data-start="1831" data-end="1847"
                            >95% variance</strong
                          >, we do:
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1856"
                          data-end="2047"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span># Find number of components needed for 95% variance
n_components_95 = np.argmax(cumulative_variance &gt;= 0.95) + 1
print(f"Optimal dimensions for 95% variance: {n_components_95}")
</span></code></div></div></pre>
                        <p data-start="2048" data-end="2142">
                          ✔ This gives
                          <strong data-start="2061" data-end="2095"
                            >the exact number of dimensions</strong
                          >
                          that keep 95% of the original data's variance.
                        </p>
                        <hr data-start="2144" data-end="2147" />
                        <h2 data-start="2149" data-end="2177">
                          <strong data-start="2152" data-end="2177"
                            >Why Does This Matter?</strong
                          >
                        </h2>
                        <p data-start="2178" data-end="2509">
                          ✅
                          <strong data-start="2180" data-end="2238"
                            >If
                            <code data-start="2185" data-end="2206"
                              >cumulative_variance</code
                            >
                            reaches 95% at 200 dimensions</strong
                          >, you should reduce to
                          <strong data-start="2261" data-end="2269"
                            >200D</strong
                          >
                          instead of guessing (e.g., assuming 128D is
                          enough).<br data-start="2322" data-end="2325" />✅
                          <strong data-start="2327" data-end="2367"
                            >Helps balance accuracy &amp; performance</strong
                          >
                          → Fewer dimensions =
                          <strong data-start="2389" data-end="2427"
                            >faster search, lower storage costs</strong
                          >.<br data-start="2428" data-end="2431" />✅
                          <strong data-start="2433" data-end="2462"
                            >Prevents information loss</strong
                          >
                          by ensuring
                          <strong data-start="2475" data-end="2506"
                            >enough variance is retained</strong
                          >.
                        </p>
                        <p
                          data-start="2511"
                          data-end="2619"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like me to generate a
                          <strong data-start="2543" data-end="2557"
                            >Scree Plot</strong
                          >
                          to visualize how variance accumulates across
                          dimensions? 📊🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-28"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="060dc5b6-fe16-4b53-af82-5a75ace3dabc"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          explain this line then n_components_95 =
                          np.argmax(cumulative_variance &gt;= 0.95) + 1
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-29"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="37f35d0f-4126-492a-89e3-fd3b7f47e04f"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <h3 data-start="0" data-end="33">
                          <strong data-start="4" data-end="33"
                            >Explanation of This Line:</strong
                          >
                        </h3>
                        <pre
                          class="!overflow-visible"
                          data-start="34"
                          data-end="108"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>n_components_95 = np.argmax(cumulative_variance &gt;= 0.95) + 1
</span></code></div></div></pre>
                        <p data-start="109" data-end="221">
                          This line
                          <strong data-start="119" data-end="161"
                            >finds the minimum number of dimensions</strong
                          >
                          required to retain
                          <strong data-start="181" data-end="213"
                            >at least 95% of the variance</strong
                          >
                          in PCA.
                        </p>
                        <hr data-start="223" data-end="226" />
                        <h2 data-start="228" data-end="257">
                          <strong data-start="231" data-end="257"
                            >Step-by-Step Breakdown</strong
                          >
                        </h2>
                        <h3 data-start="258" data-end="298">
                          <strong data-start="262" data-end="298"
                            >1.
                            <code data-start="267" data-end="296"
                              >cumulative_variance &gt;= 0.95</code
                            ></strong
                          >
                        </h3>
                        <ul data-start="299" data-end="567">
                          <li data-start="299" data-end="435">
                            <code data-start="301" data-end="322"
                              >cumulative_variance</code
                            >
                            is a
                            <strong data-start="328" data-end="343"
                              >numpy array</strong
                            >
                            where each value represents
                            <strong data-start="372" data-end="403"
                              >the total variance retained</strong
                            >
                            up to that principal component.
                          </li>
                          <li data-start="436" data-end="567">
                            <code data-start="438" data-end="467"
                              >cumulative_variance &gt;= 0.95</code
                            >
                            creates a
                            <strong data-start="478" data-end="495"
                              >boolean array</strong
                            >
                            that tells us
                            <strong data-start="510" data-end="566"
                              >which components have retained at least 95%
                              variance</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="569" data-end="606">
                          📌
                          <strong data-start="572" data-end="606"
                            >Example:
                            <code data-start="583" data-end="604"
                              >cumulative_variance</code
                            ></strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="607"
                          data-end="730"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>cumulative_variance = np.array([0.40, 0.65, 0.80, 0.90, 0.95, 0.97, 0.99])
print(cumulative_variance &gt;= 0.95)
</span></code></div></div></pre>
                        <p data-start="731" data-end="759">
                          📌
                          <strong data-start="734" data-end="759"
                            >Output (Boolean Mask)</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="760"
                          data-end="817"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none"></div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre"><span>[False, False, False, False,  True,  True,  True]
</span></code></div></div></pre>
                        <p data-start="818" data-end="915">
                          👉 This tells us that
                          <strong data-start="840" data-end="914"
                            >the 5th component (index 4) is the first one that
                            reaches 95% variance</strong
                          >.
                        </p>
                        <hr data-start="917" data-end="920" />
                        <h3 data-start="922" data-end="973">
                          <strong data-start="926" data-end="973"
                            >2.
                            <code data-start="931" data-end="971"
                              >np.argmax(cumulative_variance &gt;= 0.95)</code
                            ></strong
                          >
                        </h3>
                        <ul data-start="974" data-end="1130">
                          <li data-start="974" data-end="1063">
                            <code data-start="976" data-end="989"
                              >np.argmax()</code
                            >
                            <strong data-start="990" data-end="1011"
                              >returns the index</strong
                            >
                            of the
                            <strong data-start="1019" data-end="1041"
                              >first
                              <code data-start="1027" data-end="1033"
                                >True</code
                              >
                              value</strong
                            >
                            in the boolean array.
                          </li>
                          <li data-start="1064" data-end="1130">
                            This tells us
                            <strong data-start="1080" data-end="1129"
                              >the first component that reaches 95%
                              variance</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="1132" data-end="1147">
                          📌
                          <strong data-start="1135" data-end="1147"
                            >Example:</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1148"
                          data-end="1253"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>index = np.argmax([False, False, False, False, True, True, True])
print(index)  # Output: 4
</span></code></div></div></pre>
                        <p data-start="1254" data-end="1339">
                          👉 The
                          <strong data-start="1261" data-end="1290"
                            >4th index (5th component)</strong
                          >
                          is the first one where variance
                          <strong data-start="1323" data-end="1338"
                            >reaches 95%</strong
                          >.
                        </p>
                        <hr data-start="1341" data-end="1344" />
                        <h3 data-start="1346" data-end="1394">
                          <strong data-start="1350" data-end="1394"
                            >3.
                            <code data-start="1355" data-end="1359">+1</code>
                            (Adjusting for 0-Based Indexing)</strong
                          >
                        </h3>
                        <p data-start="1395" data-end="1543">
                          Since
                          <strong data-start="1401" data-end="1432"
                            >Python arrays are 0-indexed</strong
                          >
                          (the first element is at index
                          <code data-start="1464" data-end="1467">0</code>), we
                          add
                          <code data-start="1477" data-end="1481">+1</code> to
                          <strong data-start="1485" data-end="1542"
                            >convert the index to the correct number of
                            components</strong
                          >.
                        </p>
                        <p data-start="1545" data-end="1569">
                          📌
                          <strong data-start="1548" data-end="1569"
                            >Final Calculation</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1570"
                          data-end="1680"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>n_components_95 = np.argmax(cumulative_variance &gt;= 0.95) + 1
print(n_components_95)  # Output: 5
</span></code></div></div></pre>
                        <p data-start="1681" data-end="1755">
                          👉 This means
                          <strong data-start="1695" data-end="1754"
                            >keeping 5 principal components will retain 95%
                            variance</strong
                          >.
                        </p>
                        <hr data-start="1757" data-end="1760" />
                        <h2 data-start="1762" data-end="1782">
                          <strong data-start="1765" data-end="1782"
                            >Final Summary</strong
                          >
                        </h2>
                        <pre
                          class="!overflow-visible"
                          data-start="1783"
                          data-end="1857"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>n_components_95 = np.argmax(cumulative_variance &gt;= 0.95) + 1
</span></code></div></div></pre>
                        <p data-start="1858" data-end="2097">
                          ✅
                          <strong data-start="1860" data-end="1903"
                            >Finds the smallest number of dimensions</strong
                          >
                          needed to keep
                          <strong data-start="1919" data-end="1944"
                            >at least 95% variance</strong
                          >.<br data-start="1945" data-end="1948" />✅
                          <strong data-start="1950" data-end="1996"
                            >Uses
                            <code data-start="1957" data-end="1970"
                              >np.argmax()</code
                            >
                            to find the first index</strong
                          >
                          where cumulative variance reaches 0.95.<br
                            data-start="2036"
                            data-end="2039"
                          />✅
                          <strong data-start="2041" data-end="2094"
                            >Adds
                            <code data-start="2048" data-end="2052">+1</code>
                            because Python indexing starts from
                            <code data-start="2089" data-end="2092"
                              >0</code
                            ></strong
                          >.
                        </p>
                        <p
                          data-start="2099"
                          data-end="2177"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like me to generate a
                          <strong data-start="2131" data-end="2145"
                            >Scree Plot</strong
                          >
                          to visualize this process? 📊🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-30"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="5769d012-611b-48a9-8206-4cd9f561db3f"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          Propose a proper title for this convo
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-31"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="d7678170-a2e5-43e3-9785-6ce1fc69ccca"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <p
                          data-start="0"
                          data-end="90"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          <strong
                            data-start="0"
                            data-end="90"
                            data-is-last-node=""
                            data-is-only-node=""
                            >"Optimizing Vector Dimensions: PCA, Similarity
                            Search, and Elasticsearch Alternatives"</strong
                          >
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-32"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="fcbfac6c-80a5-46ad-be1b-d87834f32921"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          what does this mean? data = np.random.rand(1000, 512)
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-33"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="bb42b50f-51be-4f8b-b4e2-d5c6166f1f44"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <p data-start="0" data-end="11">The line:</p>
                        <pre
                          class="!overflow-visible"
                          data-start="12"
                          data-end="58"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>data = np.random.rand(1000, 512)
</span></code></div></div></pre>
                        <p data-start="59" data-end="134">
                          <strong data-start="59" data-end="106"
                            >Generates a 2D NumPy array of random values</strong
                          >
                          with shape
                          <strong data-start="118" data-end="133"
                            >(1000, 512)</strong
                          >.
                        </p>
                        <hr data-start="136" data-end="139" />
                        <h3 data-start="141" data-end="165">
                          <strong data-start="145" data-end="165"
                            >Breaking It Down</strong
                          >
                        </h3>
                        <ol data-start="166" data-end="663">
                          <li data-start="166" data-end="458">
                            <strong data-start="169" data-end="200"
                              ><code data-start="171" data-end="198"
                                >np.random.rand(1000, 512)</code
                              ></strong
                            >
                            <ul data-start="206" data-end="458">
                              <li data-start="206" data-end="305">
                                <code data-start="208" data-end="226"
                                  >np.random.rand()</code
                                >
                                is a
                                <strong data-start="232" data-end="250"
                                  >NumPy function</strong
                                >
                                that generates
                                <strong data-start="266" data-end="284"
                                  >random numbers</strong
                                >
                                between
                                <strong data-start="293" data-end="304"
                                  >0 and 1</strong
                                >.
                              </li>
                              <li data-start="309" data-end="458">
                                <code data-start="311" data-end="324"
                                  >(1000, 512)</code
                                >
                                means:
                                <ul data-start="337" data-end="458">
                                  <li data-start="337" data-end="392">
                                    <code data-start="339" data-end="345"
                                      >1000</code
                                    >
                                    → Number of
                                    <strong data-start="358" data-end="366"
                                      >rows</strong
                                    >
                                    (samples or data points).
                                  </li>
                                  <li data-start="398" data-end="455">
                                    <code data-start="400" data-end="405"
                                      >512</code
                                    >
                                    → Number of
                                    <strong data-start="418" data-end="429"
                                      >columns</strong
                                    >
                                    (features or dimensions).
                                  </li>
                                </ul>
                              </li>
                            </ul>
                          </li>
                          <li data-start="459" data-end="663">
                            <strong data-start="462" data-end="491"
                              >What Does This Represent?</strong
                            >
                            <ul data-start="495" data-end="663">
                              <li data-start="495" data-end="557">
                                <strong data-start="497" data-end="556"
                                  >A dataset with 1000 samples and 512 features
                                  per sample</strong
                                >.
                              </li>
                              <li data-start="561" data-end="663">
                                Used as
                                <strong data-start="571" data-end="606"
                                  >synthetic high-dimensional data</strong
                                >
                                for testing PCA, machine learning, or similarity
                                search.
                              </li>
                            </ul>
                          </li>
                        </ol>
                        <hr data-start="665" data-end="668" />
                        <h3 data-start="670" data-end="692">
                          <strong data-start="674" data-end="692"
                            >Example Output</strong
                          >
                        </h3>
                        <p data-start="693" data-end="704">📌 Running:</p>
                        <pre
                          class="!overflow-visible"
                          data-start="705"
                          data-end="794"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import numpy as np
data = np.random.rand(3, 5)  # Small example
print(data)
</span></code></div></div></pre>
                        <p data-start="795" data-end="812">📌 Sample Output:</p>
                        <pre
                          class="!overflow-visible"
                          data-start="813"
                          data-end="917"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none"></div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre"><span>[[0.15  0.89  0.47  0.62  0.33]
 [0.78  0.56  0.12  0.91  0.44]
 [0.67  0.21  0.76  0.35  0.59]]
</span></code></div></div></pre>
                        <ul data-start="918" data-end="1032">
                          <li data-start="918" data-end="969">
                            Each value is a
                            <strong data-start="936" data-end="968"
                              >random float between 0 and 1</strong
                            >.
                          </li>
                          <li data-start="970" data-end="1032">
                            There are
                            <strong data-start="982" data-end="1002"
                              >3 rows (samples)</strong
                            >
                            and
                            <strong data-start="1007" data-end="1031"
                              >5 columns (features)</strong
                            >.
                          </li>
                        </ul>
                        <hr data-start="1034" data-end="1037" />
                        <h3 data-start="1039" data-end="1066">
                          <strong data-start="1043" data-end="1066"
                            >Where Is This Used?</strong
                          >
                        </h3>
                        <p data-start="1067" data-end="1254">
                          ✅
                          <strong data-start="1069" data-end="1084"
                            >Testing PCA</strong
                          >
                          (e.g., reducing 512D to 128D).<br
                            data-start="1115"
                            data-end="1118"
                          />✅
                          <strong data-start="1120" data-end="1162"
                            >Simulating high-dimensional embeddings</strong
                          >
                          (e.g., vector search).<br
                            data-start="1185"
                            data-end="1188"
                          />✅
                          <strong data-start="1190" data-end="1226"
                            >Training machine learning models</strong
                          >
                          (before using real data).
                        </p>
                        <p
                          data-start="1256"
                          data-end="1351"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like me to modify this to
                          <strong data-start="1292" data-end="1320"
                            >generate real embeddings</strong
                          >
                          (e.g., using BERT or CLIP)? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-34"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="8e66f305-d7e0-4864-8e67-91fceaec7c1e"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          How many dimensions is this? data =
                          np.random.rand(1000, 512)
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-35"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="83f5b07f-6f16-4279-84a3-d8355b72c4ed"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <p data-start="0" data-end="72">
                          The number of
                          <strong data-start="14" data-end="39"
                            >dimensions (features)</strong
                          >
                          in the following line of code:
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="74"
                          data-end="120"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>data = np.random.rand(1000, 512)
</span></code></div></div></pre>
                        <p data-start="121" data-end="132">
                          <strong data-start="121" data-end="131">is 512</strong
                          >.
                        </p>
                        <hr data-start="134" data-end="137" />
                        <h3 data-start="139" data-end="170">
                          <strong data-start="143" data-end="170"
                            >Understanding the Shape</strong
                          >
                        </h3>
                        <ul data-start="171" data-end="455">
                          <li data-start="171" data-end="230">
                            <code data-start="173" data-end="200"
                              >np.random.rand(1000, 512)</code
                            >
                            creates a
                            <strong data-start="211" data-end="229"
                              >2D NumPy array</strong
                            >.
                          </li>
                          <li data-start="231" data-end="306">
                            The
                            <strong data-start="237" data-end="260"
                              >first number (1000)</strong
                            >
                            represents
                            <strong data-start="272" data-end="280"
                              >rows</strong
                            >
                            (samples or data points).
                          </li>
                          <li data-start="307" data-end="407">
                            The
                            <strong data-start="313" data-end="336"
                              >second number (512)</strong
                            >
                            represents
                            <strong data-start="348" data-end="359"
                              >columns</strong
                            >, which correspond to
                            <strong data-start="381" data-end="406"
                              >features (dimensions)</strong
                            >.
                          </li>
                          <li data-start="408" data-end="455">
                            Each
                            <strong data-start="415" data-end="431"
                              >sample (row)</strong
                            >
                            has
                            <strong data-start="436" data-end="454"
                              >512 dimensions</strong
                            >.
                          </li>
                        </ul>
                        <p data-start="457" data-end="492">
                          📌
                          <strong data-start="460" data-end="492"
                            >Checking the Shape in Python</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="493"
                          data-end="600"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import numpy as np

data = np.random.rand(1000, 512)
print(data.shape)  # Output: (1000, 512)
</span></code></div></div></pre>
                        <p data-start="601" data-end="685">
                          ✔
                          <strong data-start="603" data-end="614"
                            >Output:</strong
                          >
                          <code data-start="615" data-end="628"
                            >(1000, 512)</code
                          >
                          →
                          <strong data-start="631" data-end="685"
                            >1000 samples, each with 512 features
                            (dimensions).</strong
                          >
                        </p>
                        <hr data-start="687" data-end="690" />
                        <h3 data-start="692" data-end="713">
                          <strong data-start="696" data-end="713"
                            >Key Takeaways</strong
                          >
                        </h3>
                        <ul data-start="714" data-end="963">
                          <li data-start="714" data-end="766">
                            The dataset has
                            <strong data-start="732" data-end="765"
                              >512 dimensions per data point</strong
                            >.
                          </li>
                          <li data-start="767" data-end="892">
                            This is common in
                            <strong data-start="787" data-end="818"
                              >high-dimensional embeddings</strong
                            >
                            (e.g.,
                            <strong data-start="826" data-end="854"
                              >BERT embeddings are 768D</strong
                            >,
                            <strong data-start="856" data-end="890"
                              >CLIP image embeddings are 512D</strong
                            >).
                          </li>
                          <li data-start="893" data-end="963">
                            If used in
                            <strong data-start="906" data-end="913">PCA</strong
                            >, you might reduce
                            <strong data-start="932" data-end="947"
                              >512D → 128D</strong
                            >
                            for efficiency.
                          </li>
                        </ul>
                        <p
                          data-start="965"
                          data-end="1041"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like me to help with
                          <strong data-start="996" data-end="1037"
                            >reducing these dimensions efficiently</strong
                          >? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-36"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="c5ccb4a7-2a77-4ea4-8498-d34ff185b64a"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          In this context the dimension is considered a column?
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-37"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="4a2769b9-4287-496a-b4b0-476a5329edaf"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <p data-start="0" data-end="77">
                          Yes!
                          <strong data-start="5" data-end="60"
                            >In this context, a dimension is considered a
                            column</strong
                          >
                          in the dataset.
                        </p>
                        <hr data-start="79" data-end="82" />
                        <h3 data-start="84" data-end="120">
                          <strong data-start="88" data-end="120"
                            >Why Is a Column a Dimension?</strong
                          >
                        </h3>
                        <ul data-start="121" data-end="343">
                          <li data-start="121" data-end="343">
                            The dataset
                            <strong data-start="135" data-end="173"
                              ><code data-start="137" data-end="171"
                                >data = np.random.rand(1000, 512)</code
                              ></strong
                            >
                            is a
                            <strong data-start="179" data-end="206"
                              >matrix (2D NumPy array)</strong
                            >
                            with:
                            <ul data-start="215" data-end="343">
                              <li data-start="215" data-end="275">
                                <strong data-start="217" data-end="230"
                                  >1000 rows</strong
                                >
                                → These represent
                                <strong data-start="249" data-end="274"
                                  >samples (data points)</strong
                                >.
                              </li>
                              <li data-start="278" data-end="340">
                                <strong data-start="280" data-end="295"
                                  >512 columns</strong
                                >
                                → These represent
                                <strong data-start="314" data-end="339"
                                  >features (dimensions)</strong
                                >.
                              </li>
                            </ul>
                          </li>
                        </ul>
                        <p data-start="344" data-end="472">
                          Each
                          <strong data-start="349" data-end="365"
                            >row (sample)</strong
                          >
                          is represented by
                          <strong data-start="384" data-end="408"
                            >512 numerical values</strong
                          >, meaning each data point exists in a
                          <strong data-start="446" data-end="471"
                            >512-dimensional space</strong
                          >.
                        </p>
                        <hr data-start="474" data-end="477" />
                        <h3 data-start="479" data-end="499">
                          <strong data-start="483" data-end="499"
                            >Key Concept:</strong
                          >
                        </h3>
                        <table data-start="500" data-end="766">
                          <thead data-start="500" data-end="556">
                            <tr data-start="500" data-end="556">
                              <th data-start="500" data-end="519">
                                <strong data-start="502" data-end="518"
                                  >Row (Sample)</strong
                                >
                              </th>
                              <th data-start="519" data-end="556">
                                <strong data-start="521" data-end="554"
                                  >Columns (Dimensions/Features)</strong
                                >
                              </th>
                            </tr>
                          </thead>
                          <tbody data-start="599" data-end="766">
                            <tr data-start="599" data-end="654">
                              <td>Sample 1</td>
                              <td>[0.15, 0.89, 0.47, ..., 0.33] (512D)</td>
                            </tr>
                            <tr data-start="655" data-end="710">
                              <td>Sample 2</td>
                              <td>[0.78, 0.56, 0.12, ..., 0.44] (512D)</td>
                            </tr>
                            <tr data-start="711" data-end="766">
                              <td>Sample 3</td>
                              <td>[0.67, 0.21, 0.76, ..., 0.59] (512D)</td>
                            </tr>
                          </tbody>
                        </table>
                        <p data-start="768" data-end="849">
                          Each
                          <strong data-start="773" data-end="795"
                            >column (dimension)</strong
                          >
                          represents a
                          <strong data-start="809" data-end="820"
                            >feature</strong
                          >
                          of the data, which could be:
                        </p>
                        <ul data-start="850" data-end="1025">
                          <li data-start="850" data-end="901">
                            <strong data-start="852" data-end="878"
                              >Word embeddings in NLP</strong
                            >
                            (e.g., BERT uses 768D)
                          </li>
                          <li data-start="902" data-end="952">
                            <strong data-start="904" data-end="929"
                              >Image feature vectors</strong
                            >
                            (e.g., CLIP uses 512D)
                          </li>
                          <li data-start="953" data-end="1025">
                            <strong data-start="955" data-end="981"
                              >Tabular numerical data</strong
                            >
                            (e.g., sales data with multiple attributes)
                          </li>
                        </ul>
                        <p data-start="1027" data-end="1063">
                          📌
                          <strong data-start="1030" data-end="1063"
                            >Checking Dimensions in Python</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1064"
                          data-end="1163"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>print(f"Shape of data: {data.shape}")
print(f"Number of dimensions: {data.shape[1]}")
</span></code></div></div></pre>
                        <p data-start="1164" data-end="1179">
                          ✔
                          <strong data-start="1166" data-end="1177"
                            >Output:</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1180"
                          data-end="1240"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none"></div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre"><span>Shape of data: (1000, 512)
Number of dimensions: 512
</span></code></div></div></pre>
                        <hr data-start="1241" data-end="1244" />
                        <h3 data-start="1246" data-end="1267">
                          <strong data-start="1250" data-end="1267"
                            >Final Answer:</strong
                          >
                        </h3>
                        <p data-start="1268" data-end="1501">
                          ✅
                          <strong data-start="1270" data-end="1332"
                            >Yes, in this context, a dimension is the same as a
                            column.</strong
                          ><br data-start="1332" data-end="1335" />✅ Each
                          <strong data-start="1342" data-end="1349">row</strong>
                          represents a
                          <strong data-start="1363" data-end="1377"
                            >data point</strong
                          >
                          in a
                          <strong data-start="1383" data-end="1408"
                            >512-dimensional space</strong
                          >.<br data-start="1409" data-end="1412" />✅ Each
                          <strong data-start="1419" data-end="1441"
                            >column (dimension)</strong
                          >
                          represents an
                          <strong data-start="1456" data-end="1478"
                            >individual feature</strong
                          >
                          describing the data.
                        </p>
                        <p
                          data-start="1503"
                          data-end="1574"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like help
                          <strong data-start="1523" data-end="1560"
                            >visualizing high-dimensional data</strong
                          >
                          using PCA? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-38"
        data-scroll-anchor="false"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="user"
                    data-message-id="65e5d369-709e-4acf-a7de-0e0adde8906c"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden items-end rtl:items-start"
                    >
                      <div
                        class="relative max-w-[var(--user-chat-width,70%)] rounded-3xl bg-token-message-surface px-5 py-2.5"
                      >
                        <div class="whitespace-pre-wrap">
                          I thought 512 dimensions means that there are 512
                          nested arrays
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
      <article
        class="w-full text-token-text-primary focus-visible:outline-2 focus-visible:outline-offset-[-4px]"
        dir="auto"
        data-testid="conversation-turn-39"
        data-scroll-anchor="true"
      >
        <div class="m-auto text-base py-[18px] px-6">
          <div
            class="mx-auto flex flex-1 text-base gap-4 md:gap-5 lg:gap-6 md:max-w-3xl"
          >
            <div
              class="group/conversation-turn relative flex w-full min-w-0 flex-col agent-turn @xs/thread:px-0 @sm/thread:px-1.5 @md/thread:px-4"
            >
              <div class="flex-col gap-1 md:gap-3">
                <div class="flex max-w-full flex-col flex-grow">
                  <div
                    data-message-author-role="assistant"
                    data-message-id="bf4a381c-95b8-4f08-9a15-d1c605ceb82a"
                    dir="auto"
                    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 whitespace-normal break-words text-start [.text-message+&amp;]:mt-5"
                    data-message-model-slug="gpt-4o"
                  >
                    <div
                      class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]"
                    >
                      <div
                        class="markdown prose w-full break-words dark:prose-invert dark"
                      >
                        <p data-start="0" data-end="208">
                          I see the confusion!
                          <strong data-start="21" data-end="87"
                            >512 dimensions in this context does NOT mean 512
                            nested arrays</strong
                          >. Instead, it means
                          <strong data-start="107" data-end="207"
                            >each row (data point) has 512 numerical values
                            (features), each stored as a column in a 2D
                            array</strong
                          >.
                        </p>
                        <hr data-start="210" data-end="213" />
                        <h2 data-start="215" data-end="258">
                          <strong data-start="218" data-end="258"
                            >Clarifying the Concept of Dimensions</strong
                          >
                        </h2>
                        <h3 data-start="259" data-end="314">
                          <strong data-start="263" data-end="314"
                            >1. Understanding "512 Dimensions" in a 2D
                            Array</strong
                          >
                        </h3>
                        <p data-start="315" data-end="457">
                          When we say
                          <strong data-start="327" data-end="347"
                            >"512 dimensions"</strong
                          >, we mean that each
                          <strong data-start="367" data-end="398"
                            >row (sample) has 512 values</strong
                          >, representing its position in a
                          <strong data-start="431" data-end="456"
                            >512-dimensional space</strong
                          >.
                        </p>
                        <p data-start="459" data-end="522">
                          📌
                          <strong data-start="462" data-end="504"
                            >Example:
                            <code data-start="473" data-end="502"
                              >data = np.random.rand(3, 5)</code
                            ></strong
                          >
                          (A small dataset)
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="523"
                          data-end="627"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>import numpy as np
data = np.random.rand(3, 5)  # 3 samples, 5 dimensions each
print(data)
</span></code></div></div></pre>
                        <p data-start="628" data-end="650">
                          ✔
                          <strong data-start="630" data-end="650"
                            >Output (Example)</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="651"
                          data-end="841"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none"></div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre"><span>[[0.15  0.89  0.47  0.62  0.33]   # 1st row (1st sample, 5D)
 [0.78  0.56  0.12  0.91  0.44]   # 2nd row (2nd sample, 5D)
 [0.67  0.21  0.76  0.35  0.59]]  # 3rd row (3rd sample, 5D)
</span></code></div></div></pre>
                        <h3 data-start="842" data-end="863">
                          <strong data-start="846" data-end="863"
                            >Key Takeaways</strong
                          >
                        </h3>
                        <ul data-start="864" data-end="1019">
                          <li data-start="864" data-end="909">
                            <strong data-start="866" data-end="878"
                              >Each row</strong
                            >
                            =
                            <strong data-start="881" data-end="908"
                              >one data point (sample)</strong
                            >.
                          </li>
                          <li data-start="910" data-end="958">
                            <strong data-start="912" data-end="927"
                              >Each column</strong
                            >
                            =
                            <strong data-start="930" data-end="957"
                              >one feature (dimension)</strong
                            >.
                          </li>
                          <li data-start="959" data-end="1019">
                            <strong data-start="961" data-end="1018"
                              >The dataset is a 2D array (matrix), not nested
                              arrays</strong
                            >.
                          </li>
                        </ul>
                        <hr data-start="1021" data-end="1024" />
                        <h3 data-start="1026" data-end="1084">
                          <strong data-start="1030" data-end="1084"
                            >2. If It Were Nested Arrays (Incorrect
                            Assumption)</strong
                          >
                        </h3>
                        <p data-start="1085" data-end="1230">
                          You were thinking
                          <strong data-start="1103" data-end="1123"
                            >"512 dimensions"</strong
                          >
                          means
                          <strong data-start="1130" data-end="1176"
                            >each data point contains 512 nested arrays</strong
                          >
                          (which is incorrect). That would look something like:
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1231"
                          data-end="1319"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>data = [[ [...], [...], ..., [...]] ]  # 512 nested arrays inside each row
</span></code></div></div></pre>
                        <p data-start="1320" data-end="1417">
                          That structure is
                          <strong data-start="1338" data-end="1345">not</strong>
                          how vector data is represented in machine learning or
                          vector databases.
                        </p>
                        <hr data-start="1419" data-end="1422" />
                        <h3 data-start="1424" data-end="1474">
                          <strong data-start="1428" data-end="1474"
                            >3. What Would 512 Nested Arrays Look Like?</strong
                          >
                        </h3>
                        <p data-start="1475" data-end="1646">
                          If you had
                          <strong data-start="1486" data-end="1507"
                            >512 nested arrays</strong
                          >, the shape would be
                          <strong data-start="1528" data-end="1559"
                            >(1000, 512, something_else)</strong
                          >, meaning each data point contains
                          <strong data-start="1594" data-end="1619"
                            >512 individual arrays</strong
                          >, each with further values.
                        </p>
                        <p data-start="1648" data-end="1709">
                          📌
                          <strong data-start="1651" data-end="1709"
                            >Example of a True Nested Array (Shape: 1000 × 512 ×
                            3)</strong
                          >
                        </p>
                        <pre
                          class="!overflow-visible"
                          data-start="1710"
                          data-end="1875"
                        ><div class="contain-inline-size rounded-md border-[0.5px] border-token-border-medium relative bg-token-sidebar-surface-primary dark:bg-gray-950"><div class="flex items-center text-token-text-secondary px-4 py-2 text-xs font-sans justify-between rounded-t-[5px] h-9 bg-token-sidebar-surface-primary dark:bg-token-main-surface-secondary select-none">python</div><div class="sticky top-9 md:top-[5.75rem]"><div class="absolute bottom-0 right-2 flex h-9 items-center"><div class="flex items-center rounded bg-token-sidebar-surface-primary px-2 font-sans text-xs text-token-text-secondary dark:bg-token-main-surface-secondary"><span class="" data-state="closed"></span></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="!whitespace-pre language-python"><span>nested_data = np.random.rand(1000, 512, 3)  # Each data point has 512 sub-arrays, each with 3 values
print(nested_data.shape)  # Output: (1000, 512, 3)
</span></code></div></div></pre>
                        <p data-start="1876" data-end="1949">
                          👎
                          <strong data-start="1879" data-end="1928"
                            >This is NOT how normal vector embeddings
                            work</strong
                          >
                          in machine learning.
                        </p>
                        <hr data-start="1951" data-end="1954" />
                        <h3 data-start="1956" data-end="2012">
                          <strong data-start="1960" data-end="2012"
                            >4. The Correct Understanding of "512
                            Dimensions"</strong
                          >
                        </h3>
                        <p data-start="2013" data-end="2220">
                          <strong data-start="2013" data-end="2076"
                            >✅ "512 dimensions" just means that each row has
                            512 values.</strong
                          ><br data-start="2076" data-end="2079" />There are
                          <strong data-start="2089" data-end="2125"
                            >no nested arrays inside each row</strong
                          >. It’s just a
                          <strong data-start="2139" data-end="2219"
                            >2D NumPy array (matrix) where each row is a point
                            in a 512-dimensional space</strong
                          >.
                        </p>
                        <p
                          data-start="2222"
                          data-end="2299"
                          data-is-last-node=""
                          data-is-only-node=""
                        >
                          Would you like help
                          <strong data-start="2242" data-end="2279"
                            >visualizing high-dimensional data</strong
                          >
                          in 2D using PCA? 🚀
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
    </div>
  </body>
</html>
